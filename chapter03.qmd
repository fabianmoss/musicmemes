---
jupyter: python3
subtitle: What happens if people just blindly copy?
filters:
  - shinylive
---

# Unbiased transmission {#sec-unbiased_transmission}

:::{.callout-note}
This chapter is based on "Chapter 1: Unbiased transmission" in @Acerbi2022.
:::

In this chapter, we introduce the most basic model for cultural inheritance: unbiased transmission. This process quite literally corresponds to randomly copying traits from previous generations, without any further distinctions and constraints. While this is obviously a too reductive model for how cultural transmission works, it is ideally suited to get us started with the enterprise of modeling evolutionary processes involving random variation.

First we import some modules.

```{python}
import numpy as np
import pandas as pd
```

Because we will model evolutionary processes that are not strictly deterministic, we need to simulate variations due to random change.
For this, we can use the _default random number generator_ from the NumPy library and store it in the variable `rng`. 

```{python}
rng = np.random.default_rng(seed=42)
```

Next, we define some basic variables that we take into account for our first model. We consider a population of $N=100$ individuals as well as a time-frame of $t_{max}=100$ generations. 

## Creating a conceptual understanding

It is a useful exercise to imagine first what we want to implement. 
This way we can check whether we have a good conceptual understanding, which 
will help us to write the code more clearly and make fewer mistakes.
The following figure shows in the upper panel a scenario, in which there is a population of `N`
individuals present in each generation. Those individuals are uniquely characterized by one of two traits: 
whether they like opera (salmon color) or not (grey).
This visualization captures all assumptions we make for our first model:

1. The number of individuals per generation does not change. 
2. Generations are disjunct, there is no overlap.
3. In the first generation, opera preference is randomly assigned to individuals.
4. In each subsequent generation, each individual randomly picks an 'ancestor' 
   from the previous generation and blindly copies that individual's preference for opera.

The models throughout this book assume that invidivuals in one generation
learn from individuals of (only) the previous generation by "picking"
an older individual and adopting its traits according to some probabilistic rules built into 
the model. This means an arrow from individual `n` in generation `t`
to an individual `m` in generation `t + 1` should be interpreted as:
"Individual `m` learns from individual `m`" and not the other way around
(because information is flowing from generation `t` to `t+1`).

![A conceptual depiction of unbiased transmission.](img/chains.png)

The lower plot shows, at each time, point the percentage of individuals 
having a preference for opera. A logical consequence of this process is that each transmission chain (each sequence of directly connected arrows) will pass through one and only one trait (color).

We can make further observations from this initial example.
In the first generation, there are two individuals who do like opera 
and three who don't. In subsequent generations, the proporation of these 
two traits changes. It is not _monotonic_ (it goes both up and down), as the 
red line shows. But in the sixth generation, everything changes.
Individuals in the sixth generation _could_ inherit their trait 
from the one individual in generation 5 that likes opera. But because 
they randomly pick their ancestors, that individual is not among the ancestors.
Consequently, no individual in generation 6 likes opera. 
This also means that, from now on, now one will ever like opera again. 
Opera fans have become extinct.

We can see that transmission of information is still going on:
there are arrows between generations, so individuals still receive 
information from the previous generation. But nothing changes. 
That means that we _do have_ transmission of information, but we would not 
anymore speak of it as _cultural evolution_, since the first fundamental criterion (see Chapter X), variation, is not fulfilled.

Note also, that some 'traditions' are likewise not continued.
For example, following the transmission chain of individual 0 
in the first generation to individual 2 in the second generation
to individual 3 in the third generation, we see that this latter 
individual is not picked as an ancestor by anyone from the next generation.
This individual (and all of its ancestors) have no more impact on 
future generations. Importantly, however, we are not interested 
in individual fates, but rather in population-level statistics.
That is why the lower plot only traces the proportion of individuals 
having a preference for opera. 

## Simulating a population

With this conceptual understanding in mind,
we will now look at how we can reproduce 
this model of unbiased transmission. 
Since we are assuming that new individuals 
randomly pick an ancestor, we should not assume that 
our results will be exactly the same as above,
but they should nonetheless be qualitatively similar.

```{python}
N = 100
t_max = 100
```

:::{.callout-note}
In general, we use the variable `t` to designate generation counts. 
:::

Now we create a variable `population` that will store the data about our simulated population. 
This population has either of two traits `"A"` and `"B"`, with a certain probability. 
We store all of this in a so-called 'data frame', which is a somewhat fancy, Pandas-specific term for a table. 

```{python}
population = pd.DataFrame(
    {"trait": rng.choice(["A", "B"], size=N, replace=True)}
)
```

Let's take this code apart to understand it better.
From the Pandas library, which we imported as the alias `pd`, we create a `DataFrame` object. 
The data contained in this the data frame `population` is specified via a dictionary 
that has `"trait"` as its key and a fairly complex expression starting with the 
random number generator `rng` as its value. What this value says is, from the list `["A", "B"]` 
choose randomly `N` instances with replacement (if `replace` were set to `False`, 
we could at most sample 2 values from the list). So, the data frame `population` 
should contain 100 randomly sampled values of A's and B's. Let's confirm this by looking
at the first 10 individuals of the population:

```{python}
population.head(10)
```

As you can see, `population` stores a table (many of the 100 rows are omitted here for display reasons) and a single column called 'trait'. The `.head()` method appended to the `population` data frame shows restricts the output to only the first 5 rows (0 through 4). Each row in the 'trait' column contains either the value A or B. To the left of the data frame you can see the numbers of rows explicitly spelled out. This is called the data frame's _index_. 

:::{.callout-note}
A and B are just placeholder names for any of two mutually exclusive cultural traits. These could be, for example, preference for red over white whine (ignoring people who like ros√© as well as people who have no preference). You see already here that this is a massive oversimplification of actual taste preferences. The point here is not to construct a plausible model but rather to gradually build up a simple one in order to understand well its inner workings.

It will help to pause for a moment and to think of other examples that "A" and "B" could stand for. Can you come up with a music-related one?
:::

For instance, we could say that the mutually exclusive
traits "A" and "B" correspond to "Individual likes opera" and "Individual doesn't like opera". 
People are often opinionated about opera, so we 
will stick to this example for the remainder of 
this part of the book. But be encouraged to try 
to transfer the following to different hypothetical 
scenarios. 

![Scene from ["What's opera, doc?"](https://en.wikipedia.org/wiki/What%27s_Opera,_Doc) (1957).](img/opera.jpeg)

Back to our artificial population.
We can count the number of A's and B's amongst
the individuals as follows:

```{python}
population["trait"].value_counts()
```

You can read the above code as "From the population table, select the 'trait' colum and count its values.". Since there were only two values to sample from and they were randomly (uniformly) sampled, the number of A's and the number of B's should be approximately equal. We can obtain their relative frequencies by adding setting the `normalize` keyword to `True`:

```{python}
population["trait"].value_counts(normalize=True)
```

## Tracing cultural change

We now create a second data frame `output` in which we will store the output of our model. This data frame has two columns: `generation`, which is the number of the simulated generation, and `p` which stands for "the probability of an individual of having trait A".

```{python}
output = pd.DataFrame(
    {
        "generation": np.arange(t_max, dtype=int), 
        "p": [np.nan] * t_max 
    }
)
```

The `generation` column contains all numbers from `0` to `t_max - 1`. Because we count the numbers of generations (rather than assuming a time-continuous process), we specified that numbers in this column have to be intergers (`dtype=int`). The values for the `p` column must look cryptic. It literally says: put the `np.nan` value `t_max` times into the `p` colum. `np.nan` stands for "not a number" (from the NumPy library), since we haven't assigned any values to this probability yet. 

```{python}
output.head()
```

Don't worry that both the index and the 'generation' column contain all numbers from 0 to 99. We need this later when things become more involved.

As the saying goes, from nothing comes nothing, so we have to start somewhere, meaning that we need to assume that the initial probability of having trait A in our population is an actual number. The most sensible thing is to start with the proportions of A and B in our sampled population as a starting value. 

So, we approximate the probability of an individual having trait A with the relative frequency of trait A in the population: 

```{python}
population["trait"].value_counts(normalize=True)["A"]
```

You already know this code from above, we just added the `["A"]` part at the end to select only the relative frequencies of trait A. We want to set this as the value of `p` of the first generation. This can be achieved with the `.loc` (location) method:

```{python}
output.loc[0, "p"] = population["trait"].value_counts(normalize=True)["A"]
```

In words, this reads: "Set location 0 (first row) in the `p` column of the `output` data frame to the relative frequency of the trait 'A' in the population."

## Iterating over generations

Recall that we are trying to observe cultural change over the course of `t_max = 100` generations. We thus simply repeat what we just did for the first generation: based on the relative frequencies of A's and B's in the previous generation, we sample the traits of 100 new individuals for the next generation.

```{python}
for t in range(1, t_max):
    # Copy the population data frame to `previous_population`
    previous_population = population.copy()
  
    # Randomly copy from previous generation's individuals
    new_population = previous_population["trait"].sample(N, replace=True).to_frame()
    
    # Get p and put it into the output slot for this generation t
    output.loc[t, "p"] = new_population[ new_population["trait"] == "A"].shape[0] / N
```

This procedure assignes a probability of having trait "A" for each generation (each row of the `p` colum is filled now):

```{python}
output.head()
```

To make things easier, we wrap the above code in a function that we'll call `unbiased_transmission` that can take different values for the population size `N` and number of generations `t_max` as parameters.
The code below is exactly the same as above.
```{python}
def unbiased_transmission_1(N, t_max):
    population = pd.DataFrame({"trait": rng.choice(["A", "B"], size=N, replace=True)})

    output = pd.DataFrame({"generation": np.arange(t_max, dtype=int), "p": [np.nan] * t_max })

    output.loc[0, "p"] = population[ population["trait"] == "A"].shape[0] / N

    for t in range(1, t_max):
        # Copy the population tibble to previous_population tibble
        previous_population = population.copy()
    
        # Randomly copy from previous generation's individuals
        new_population = previous_population["trait"].sample(N, replace=True).to_frame()
        
        # Get p and put it into the output slot for this generation t
        output.loc[t, "p"] = new_population[ new_population["trait"] == "A"].shape[0] / N
    
    return output
```

```{python}
data_model = unbiased_transmission_1(N=100, t_max=200)
```

```{python}
def plot_single_run(data_model):
    data_model["p"].plot(ylim=(0,1))
```

```{python}
#| fig-cap: Single run of the unbiased transmission model for a population of $N=100$ individuals and $t_{max}=200$ generations.
plot_single_run(data_model)
```

```{python}
data_model = unbiased_transmission_1(N=10_000, t_max=200)
```

```{python}
#| fig-cap: Single run of the unbiased transmission model for a population of $N=10,000$ individuals and $t_{max}=200$ generations.
plot_single_run(data_model)
```

<!-- TODO: Implement interactive code as shiny app -->
<!-- Try playing around with different parameter values. Bear in mind, however, that too large population sizes will take for ever to compute.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 420

from shiny import App, render, ui
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

rng = np.random.default_rng()

app_ui = ui.page_fluid(
    ui.layout_sidebar(
        ui.panel_sidebar(
            ui.input_slider("N", "population size", 100, 10000, 1000, step=500),
            ui.input_slider("t_max", "generations", 1, 50, 20, step=1),
        ),
        ui.panel_main(
            ui.output_plot("plot"),
        ),
    ),
)

def server(input, output, session):
    @output
    @render.plot(alt="First model")
    def plot():
        plt.plot([1,2 * input.t_max() ], [3 + input.N(),4], marker="o")
        # def unbiased_transmission_1(N, t_max):
        #     population = pd.DataFrame({"trait": rng.choice(["A", "B"], size=N, replace=True)}, index=[0])

        #     output = pd.DataFrame({"generation": np.arange(t_max, dtype=int), "p": [np.nan] * t_max }, index=[0])

        #     output.loc[0, "p"] = population[ population["trait"] == "A"].shape[0] / N

        #     for t in range(1, t_max):
        #         # Copy the population tibble to previous_population tibble
        #         previous_population = population.copy()
            
        #         # Randomly copy from previous generation's individuals
        #         new_population = previous_population["trait"].sample(N, replace=True).to_frame()
                
        #         # Get p and put it into the output slot for this generation t
        #         output.loc[t, "p"] = new_population[ new_population["trait"] == "A"].shape[0] / N
            
        #     return output
        
        # data_model = unbiased_transmission_1(N=None, t_max=None)

        # def plot_single_run(data_model):
        #     data_model["p"].plot(ylim=(0,1))

        # plot_single_run(data_model)

app = App(app_ui, server)

``` -->

Now, let's adapt the code somewhat.

```{python}
def unbiased_transmission_2(N, t_max, r_max):
    output = pd.DataFrame({
        "generation" : np.tile(np.arange(t_max), r_max),
        "p" : [ np.nan ] * t_max * r_max,
        "run" : np.repeat(np.arange(r_max), t_max)
    })

    for r in range(r_max):
        # Create first generation
        population = pd.DataFrame({"trait": rng.choice(["A", "B"], size=N, replace=True)})

        # Add first generation's p for run r
        output.loc[ r * t_max, "p"] = population[ population["trait"] == "A" ].shape[0] / N

        # For each generation 
        for t in range(1,t_max):
            # Copy individuals to previous_population DataFrame
            previous_population = population.copy()

            # Randomly compy from previous generation 
            population = population["trait"].sample(N, replace=True).to_frame()

            # Get p and put it into output slot for this generation t and run r
            output.loc[r * t_max + t, "p"] = population[ population["trait"] == "A" ].shape[0] / N

    return output
```

```{python}
unbiased_transmission_2(100, 100, 3).head()
```

::: {.callout-tip}
Why could we append `.head()` to the `unbiased_transmission_2` function?
:::

```{python}
data_model = unbiased_transmission_2(N=100, t_max=200, r_max=5)
```

```{python}
def plot_multiple_runs(data_model):
    groups = data_model.groupby("run")
    for _, g in groups:
        g.index = g["generation"]
        g["p"].plot(lw=.5, ylim=(0,1))

    data_model.groupby("generation")["p"].mean().plot(c="k", lw="1")
```

```{python}
#| fig-cap: Multiple runs of the unbiased transmission model for a population of $N=100$ individuals, with average (black line).
plot_multiple_runs(data_model)
```

```{python}
data_model = unbiased_transmission_2(N=10_000, t_max=200, r_max=5)
```

```{python}
#| fig-cap: Multiple runs of the unbiased transmission model for a population of $N=10,000$ individuals, with average (black line).
plot_multiple_runs(data_model)
```

```{python}
def unbiased_transmission_3(N, p_0, t_max, r_max):
    output = pd.DataFrame({
        "generation" : np.tile(np.arange(t_max), r_max),
        "p" : [ np.nan ] * t_max * r_max,
        "run" : np.repeat(np.arange(r_max), t_max)
    })

    for r in range(r_max):
        # Create first generation
        population = pd.DataFrame({"trait": rng.choice(["A", "B"], size=N, replace=True, p=[p_0, 1 - p_0])})

        # Add first generation's p for run r
        output.loc[ r * t_max, "p"] = population[ population["trait"] == "A" ].shape[0] / N

        # For each generation 
        for t in range(1,t_max):
            # Copy individuals to previous_population DataFrame
            previous_population = population

            # Randomly compy from previous generation 
            population = population["trait"].sample(N, replace=True).to_frame()

            # Get p and put it into output slot for this generation t and run r
            output.loc[r * t_max + t, "p"] = population[ population["trait"] == "A" ].shape[0] / N

    return output
```

```{python}
data_model = unbiased_transmission_3(10_000, p_0=.2, t_max=200, r_max=5)
plot_multiple_runs(data_model)
```

The lesson here is that, if the population is large enough and individuals randomly copy from 
the previous generation, the average frequency of traits in the population will not significantly change.
