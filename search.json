[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Music Memes",
    "section": "",
    "text": "Welcome\nOn these pages you will learn about cultural evolution and music. The overall aim is to attain a basic understanding of formal models in cultural evolution and learn about several recent approaches that apply them to the domain of music.\nWe start with a minimal introduction to the Python programming language that covers the necessary basic skills in order to follow the remainder of the book. Then, we summarize some general ideas about music and cultural evolution.\nSubsequently, we follow the excellent learning path for computational models in cultural evolution provided by the book Individual-based models of cultural evolution: A step-by-step guide using R (Acerbi et al., 2022). These pages comprise a translation of this resource to Python. Finally, we will review and discuss a number of recent publications on music and cultural evolution in the advanced topics section at the end.\n\n\n\n\n\n\nIf you want to refer to this resource, please cite it as appropriately, e.g.:\nMoss, F. C. (2023). Music memes: Understanding music transmission processes through cultural evolution modeling. https://fabianmoss.github.io/musicmemes\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe material on this page has been adapted and designed for my musicology research seminar “Music Memes: quantitative approaches and theories of cultural transmission of music” at Würzburg University, Germany.\nNote that the materials here are still under development. Please inform me if you notice any errors or other issues.\n\n\n\nAcknowledgements\nI am grateful for encouragement from Alberto Acerbi to continue working on my Python translation of his book and many helpful comments.\n\n\n\n\nAcerbi, A., Mesoudi, A., & Smolla, M. (2022). Individual-based models of cultural evolution: A step-by-step guide using R. Routledge. https://acerbialberto.com/IBM-cultevo/",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Musical Memes\nThe term ‘meme’ has been adopted by everyday language and is usually associated with a certain image, superimposed with text, that is shared on the internet and has a particular contextual meaning.\nOriginally, however, the definition of ‘meme’ was much broader. Towards the end of The Selfish Gene, Richard Dawkins introduced the concept as follows:\nHere, we adopt this original, more extensive concept of memes that, as we will discover, comprises the more or less funny internet pictures as a special case.\nThe field of cultural evolution emerged in the 1980’s (e.g., Boyd & Richerson, 1985; Cavalli-Sforza & Feldman, 1981), and has, in parallel with the advancement of computational facilities, gained momentum. Theories on cultural evolution share many facets with approaches on memetics (Aunger, 2001; Blackmore, 2000; Dawkins, 1976; Howe & Windram, 2011), a field that has also been applied to the case of music (Jan, 2016).\nIn recent years, several approaches have attempted to apply formal models from cultural evolution to the domain of music.\nIn the present context, we first introduce some central ideas of cultural evolution and review a few major publications for the domain of music.\nA few selected important contributions are:",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#musical-memes",
    "href": "intro.html#musical-memes",
    "title": "1  Introduction",
    "section": "",
    "text": "A musical meme. This image itself has been ended up here after being passed along a chain of cultural transmission processes involving several social media: a Google search for “music meme” led to an image result pointing to Classic FM, which, in turn, had taken this image from the instagram page of user @musicmemes.for.supertonicteens.\n\n\n\n\n“We need […] a noun that conveys the idea of a unit of cultural transmission, or a unit of imitation. ‘Mimeme’ comes from a suitable Greek root,1 but I want a monosyllable word that sounds a bit like ‘gene’. I hope my classicist friends will forgive me if I abbreviate mimeme to meme.” Dawkins (1976, p. 192)\n\n\n\n\n\n\n\n“Cultural Transmission and Evolution: A Quantitative Approach” (Cavalli-Sforza & Feldman, 1981)\n“Culture and the Evolutionary Process” (Boyd & Richerson, 1985)\n“The Memetics of Music” (Jan, 2016)\n“Cultural Evolution and Music” (Youngblood et al., forthcoming)\n“Cultural Evolution of Music” (Savage, 2019)",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#conditions-for-an-evolutionary-system",
    "href": "intro.html#conditions-for-an-evolutionary-system",
    "title": "1  Introduction",
    "section": "1.2 Conditions for an evolutionary system",
    "text": "1.2 Conditions for an evolutionary system\n\nvariation\nselection\nretention\n\nSee Blackmore (2000), Chapter 2 “Universal Darwinism” for an excellent introduction.\nThese factors are, coincidentally, also central in the definition of folk music put forward by the International Folk Music Council:\n\n“Folk music is the product of a musical tradition that has been evolved through the process of oral transmission. The factors that shape the tradition are: (i) continuity which links the present with the past; (ii) variation which springs from the creative impulse of the individual or the group; and (iii) selection by the community, which determines the form or forms in which the music survives.” (International Folk Music Council, 1955, p. 23; see also Karpeles, 1955)\n\nIt is interesting that many contempory definitions of music, for example the aphoristic “humanly structured sound” one, do not refer explicitly to modes or conditions of transmission.",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#music-in-human-evolution",
    "href": "intro.html#music-in-human-evolution",
    "title": "1  Introduction",
    "section": "1.3 Music in human evolution",
    "text": "1.3 Music in human evolution\nThis book is about the cultural evolution of music. It has to be mentioned, however, that there is a large body of research on the biological evolution of music. This research asks questions about with which evolutionary advantages music endowed early humans, whether it is something that we share with other animals or whether it makes us unique. Some hold the view that music has no particularly relevant evolutionary function at all (Pinker, 1997), while others see in it a key to our success as a species (Cross, 2016).\nWhatever the true role of music in the evolutionary history of humanity may have been, it is most certainly a fascinating topic to reflect upon. After all, human musical activity with dedicated instruments can be traced back at least 20,000 years, although it seems more than likely that human ‘musicking’ dates back much further, since our own bodies and voices already provided us with excellent musical instruments long before the first instruments have been devised.\nIf you are interested in learning more about biological-evolutionary aspects of music, I highly recommend to read, e.g. Wallin et al. (2001), Morley (2013), Tomlinson (2018), and Honing (2018).",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#history-and-cultural-evolution",
    "href": "intro.html#history-and-cultural-evolution",
    "title": "1  Introduction",
    "section": "1.4 History and cultural evolution",
    "text": "1.4 History and cultural evolution\nDocumenting, describing, and interpreting changes in human culture is what historians do. Accordingly, changes in music belong to the field of music history, or historical musicology. However, most historians would probably be hesitant to employ models, or worse: formal models, in order to describe historical processes. The dictum “history doesn’t repeat itself” seems to raise a fundamental argument against such endeavors that aim at explaining cultural or historical changes by means of underlying latent ‘forces’. Modeling, in that view, seems to erroneously assume that history is teleological – directed towards a predetermined goal.\nOn the other hand, it is undeniable that there are many aspects of culture exhibit regularities and ‘progresses’ that are hard to explain if there are no guiding processes. Defining “culture” is, of course, yet another difficult enterprise. Here, I follow more or less the definition of Boyd & Richerson (1985, p. 33): “Culture is information capable of affecting individuals’ phenotypes which they acquire from other conspecifics by teaching or imitation [emphasis mine]”.2 The part important to us here is “by teaching or imitation”, which is meant to imply: not by genetic inheritance. If humans can transmit information by other means than genetic inheritance, and if these transmission processes continue over many generations, then they are worth studying. Rest assured, the assumption of a hidden goal towards which all cultural processes are directed is not needed at all! I hope that you will share this conviction after working through this material.",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#schema-theory",
    "href": "intro.html#schema-theory",
    "title": "1  Introduction",
    "section": "1.5 Schema theory",
    "text": "1.5 Schema theory\nWhat are the units (the basic memes) in music? Schema theory proposes a set of more or less fixed patterns, ‘schemata’, that underly a large body of music in the Baroque and Classical period (Gjerdingen, 2007).\nAre schemata really memes? I would think not because they can not mutate freely without losing their meaning. I rather think schemata ‘fall out’ of several combinatorial possibilities to traverse the diatonic scale, and compositional mechanisms to elaborate and vary them. But this kind of variation is very different than the one we talk about in this book. Composed variation is very regular, not random, and relying both on the melodic and metric structures in which it unfolds. Those, however, could very well be understood as environments with very strong constraints (being ‘out-of-scale’ or ‘off-beat’).",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#basic-cultural-inheritance-mechanisms",
    "href": "intro.html#basic-cultural-inheritance-mechanisms",
    "title": "1  Introduction",
    "section": "1.6 Basic cultural inheritance mechanisms",
    "text": "1.6 Basic cultural inheritance mechanisms\nFollowing this introduction, we introduce some minimal requirements to use Python for this course (Chapter 3).\nSubsequently, we introduce with six central mechanisms for cultural inheritance: unbiased transmission (Chapter 6), unbiased and biased mutation (Chapter 7), directly biased transmission (Chapter 9), frequency-dependent indirectly biased transmission (Chapter 10), and demonstrator-based indirectly biased transmission (Chapter 11). We follow up with a chapter on vertical and horizontal transmission (Chapter 12), and finally introduce the multiple traits model (Chapter 13). The following diagram gives an overview of these processes:\n\n\nAfter having a firm grasp on how these processes can be modeled in Python and how modeling results can be interpreted, we move to more advanced topics, and more specifically into a number of recent exciting results about cultural evolution and music. We conclude our journey (Chapter 18) with a more general discussion of the implications of cultural evolution for how we think about music, on the relevance of modeling in music research and the humanities more generally, as well as on the great potential of this approach for future research.",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#progress",
    "href": "intro.html#progress",
    "title": "1  Introduction",
    "section": "1.7 Progress?",
    "text": "1.7 Progress?\nFinally, at the end of this introduction, I want to circle back to an important issue that has been central to many concerns surrounding the application of evolutionary models to music, and to culture more generally. I am speaking about notions of progress and teleology in the sense that (cultural) evolution would be, in some sense, continuously improving. And not only that, it would be guided towards a certain goal, an optimal state of superiority. These concerns shouldn’t be disregarded too lightly, especially in view of the misunderstandings and harmful consequences they have caused. On the other hand, we also need a differentiated perspective on evolutionary theory, its assumptions, and the extent to which one might apply it to cultural phenomena. I believe that the generally critical attitude in the humanities can be, in fact, a great catalyst to weed out inappropriate notions and interpretations. However, what is needed in addition is also a certain openness to engage in discussions with scientific approaches to culture and not an unreflected (and thus uncritical) dismissal per se. The general attitude towards evolutionary thinking in biology and culture is nicely summarized in a quote by Kwame Anthony Appiah:\n\nNowadays, it is clear that one of the most distinctive marks of our species is that our inheritance is both biological and cultural. Each generation of human beings in a particular society can build on what was learned by the ones before […]. What makes us the wise species—sapiens, remember is the Latin for “wise”—is that our genes make brans that allow us to pick up things from one another that are not in our genes. (Appiah, 2018, p. 122)\n\nThis book wants to contribute to a fruitful discussion, provide entry points for humanists to better understand computational evolutionary models, and to show in which areas they have been and could be applied. After all, all of academia, the humanities and the sciences alike, are also subject of constant changes and adaptations to new environmental conditions. Music research is no exception.\n\n\n\n\nAppiah, K. A. (2018). The Lies that Bind: Rethinking Identity. W. W. Norton & Company.\n\n\nAunger, R. (2001). Darwinizing Culture: The Status of Memetics as a Science. Oxford University Press, USA. http://gen.lib.rus.ec/book/index.php?md5=7329e2aa9adcddfed967088219426193\n\n\nBlackmore, S. (2000). The Meme Machine. Oxford University Press.\n\n\nBoyd, R., & Richerson, P. J. (1985). Culture and the Evolutionary Process. The University of Chicago Press.\n\n\nCavalli-Sforza, L. L., & Feldman, M. W. (1981). Cultural Transmission and Evolution. Princeton University Press.\n\n\nCross, I. (2016). The nature of music and its evolution. In S. Hallam, I. Cross, & M. Thaut (Eds.), The Oxford Handbook of Music Psychology (2nd ed., pp. 1–20). Oxford University Press. https://doi.org/10.1093/oxfordhb/9780199298457.013.0001\n\n\nDawkins, R. (1976). The Selfish Gene. Oxford University Press.\n\n\nGjerdingen, R. O. (2007). Music in the Galant Style. Oxford University Press.\n\n\nHoning, H. (2018). On the biological basis of musicality. Annals of the New York Academy of Sciences. https://doi.org/10.1111/nyas.13638\n\n\nHowe, C. J., & Windram, H. F. (2011). Phylomemetics beyond the Gene. PLOS Biology, 9(5), e1001069. https://doi.org/10.1371/journal.pbio.1001069\n\n\nInternational Folk Music Council. (1955). Resolutions. Journal of the International Folk Music Council, 7, 23–23. https://www.jstor.org/stable/834530\n\n\nJan, S. (2016). The Memetics of Music: A Neo-Darwinian View of Musical Structure and Culture. Routledge.\n\n\nKarpeles, M. (1955). Definition of Folk Music. Journal of the International Folk Music Council, 7, 6–7. https://doi.org/10.2307/834518\n\n\nMorley, I. (2013). The Prehistory of Music. Human Evolution, Archaeology, and the Origins of Musicality. Oxford University Press.\n\n\nPinker, S. (1997). How the mind works. Norton.\n\n\nSavage, P. E. (2019). Cultural evolution of music. Palgrave Communications, 5(1), 1–16. https://doi.org/10.1057/s41599-019-0221-1\n\n\nTomlinson, G. (2018). A Million Years of Music. Princeton University Press. https://press.princeton.edu/books/paperback/9781890951528/a-million-years-of-music\n\n\nWallin, N. L., Merker, B., & Brown, S. (Eds.). (2001). The Origins of Music. MIT Press.\n\n\nYoungblood, M., Ozaki, Y., & Savage, P. E. (forthcoming). Cultural evolution and music. In J. Tehrani, J. R. Kendal, & R. L. Kendal (Eds.), Oxford Handbook of Cultural Evolution. Oxford University Press. https://psyarxiv.com/xsb7v",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Mimesis↩︎\nI encourage you to read Chapter 3 of their seminal book in order to fully capture the meaning of this quote.↩︎",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "style.html",
    "href": "style.html",
    "title": "2  Style",
    "section": "",
    "text": "2.1 A model of style in music\nA comprehensive model for theoretical treatment of style was proposed by Meyer (1989). In a nutshell, he proposes a model where certain constraints favor differnt kinds of patternings. He organises these constraints hierarchically so that this model is suitable to describe a variety of stylistic phenomena on different levels. This clear separation is more conceptual and practical only for theoretical discussions. In a real scenario, boundaries are often unclear or ambigous.",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Style</span>"
    ]
  },
  {
    "objectID": "style.html#a-model-of-style-in-music",
    "href": "style.html#a-model-of-style-in-music",
    "title": "2  Style",
    "section": "",
    "text": "2.1.1 Constraints\n\nLaws\nRules\nStrategies\n\n\n\n2.1.2 Patterns\n\nDialect\nIdiom\nIntraopus Style (here, we need a concept of “work”, but the extended “meme” concept or “memecomplex might also be useful”)\n\n\n\n\n\nMeyer, L. B. (1989). Style and Music. Theory, History, and Ideology. University of Chicago Press.",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Style</span>"
    ]
  },
  {
    "objectID": "python_primer.html",
    "href": "python_primer.html",
    "title": "3  A Python primer",
    "section": "",
    "text": "3.1 Variables and types\nVariable assignment in Python is straight-forward. You choose a name for the variable, and assign a value to it using the = operator, for example:\nx = 100\nassigns the value 100 to the variable x. If we call the variable now, we can see that it has the value we assigned to it:\nx\n\n100\nOf course, we can also assign things other than numbers, for example:\nname = \"Fabian\"\nWhat we assigned to the variable name is called a string, it has the value \"Fabian\". Strings are sequences of characters.\nWe can also assign a list of things to a variable:\nmylist = [1, 2, 3, \"A\", \"B\", \"C\"]\nLists are enclosed by square brackets. As you can see, Python allows you to store any kind of data in lists (here: integer numbers and character strings). However, it is good practice to include only items of the same type in lists—you’ll understand later why.\nAnother structured data type in python are dictionaries. Dictionaries are collections of key-value pairs. For example, a dictionary addresses could store the email addresses of certain people:\naddresses = {\n    \"Andrew\" : \"andrew@example.com\",\n    \"Zoe\" : \"zoe@example.com\"\n}\nNow, if we wanted to look up Zoe’s email address, we could to so with:\naddresses[\"Zoe\"]\n\n'zoe@example.com'",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Python primer</span>"
    ]
  },
  {
    "objectID": "python_primer.html#variables-and-types",
    "href": "python_primer.html#variables-and-types",
    "title": "3  A Python primer",
    "section": "",
    "text": "Tip\n\n\n\nNote that \"Fabian\" is enclosed by double-quotes. Why is this the case? Why could we not just type name = Fabian?",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Python primer</span>"
    ]
  },
  {
    "objectID": "python_primer.html#on-repeat",
    "href": "python_primer.html#on-repeat",
    "title": "3  A Python primer",
    "section": "3.2 On repeat",
    "text": "3.2 On repeat\nCoding something is only useful if you can’t do the job as fast or as efficient by yourself. Especially when it comes to repeating the same little thing many, many times, knowing how to code comes in handy.\nAs a simple example, imagine you want to write down all numbers from 1 to 10, or from 1 to 100, or… you get the idea. In Python, you would do it as follows:\n\nfor i in range(10):\n    print(i)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nYou see that this is not exactly what we wanted. We’re seeing numbers from 0 to 9, each one being printed on a new line. But what we wanted was all numbers from 1 to 10. Before we fix the code to produce the desired result, let’s explain the bits and pieces of the code above.\nWhat we just did was to use a so-called for-loop, probably the most common way to repeat things in Python. First we create an iterator variable i (we could have named any other variable name as well), which takes its value from the list of numbers specified by range(10). If only one number n is provided to range(n), it will enumerate all numbers from 0 to n-1. If instead two arguments are provided, the first one determines the starting number, and the second one stands for the terminating number minus one—confusing, right?\nSo, in order to enumerate all numbers from 1 to 10, we have to write range(1,11). Additionally, we can use the end keyword of the print function that allows us to print all numbers in one line, separated only by a single white space instead of each one on a new line.\n\nfor i in range(1,11):\n    print(i, end=\" \")\n\n1 2 3 4 5 6 7 8 9 10 \n\n\nVoilà!",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Python primer</span>"
    ]
  },
  {
    "objectID": "python_primer.html#just-in-case",
    "href": "python_primer.html#just-in-case",
    "title": "3  A Python primer",
    "section": "3.3 Just in case",
    "text": "3.3 Just in case\nOften we encounter a situation where we would execute some code only if certain conditions are met. In python, this is done with the if statement. For example, if we want to only print the even numbers in the range from 1 to 10, we could adapt the code from above as follows:\n\nfor i in range(1,11):\n    if i % 2 == 0:\n        print(i, \"is even\")\n\n2 is even\n4 is even\n6 is even\n8 is even\n10 is even\n\n\nYou can read this as “if the remainder of dividing i by 2 is zero, then print ‘i is even’”.\nNow, we could also want to print a similar statement in the case that i is odd:\n\nfor i in range(1,11):\n    if i % 2 == 0:\n        print(i, \"is even\")\n    else:\n        print(i, \"is odd\")\n\n1 is odd\n2 is even\n3 is odd\n4 is even\n5 is odd\n6 is even\n7 is odd\n8 is even\n9 is odd\n10 is even\n\n\nAnd, finally, we could also have more than just one condition. An if-statement allows for arbitrary many if-else clauses, with which we can formulate several different conditions by writing elif (shorthand for ‘or else if’):\n\nfor i in range(1,11):\n    if i % 2 == 0:\n        print(i, \"is even\")\n    elif i % 3 == 0:\n        print(i, \"is divisible by 3\")\n    else:\n        print(i, \"is odd\")\n\n1 is odd\n2 is even\n3 is divisible by 3\n4 is even\n5 is odd\n6 is even\n7 is odd\n8 is even\n9 is divisible by 3\n10 is even\n\n\nWe now know when a number is even and when it is divisible by 3. But what about numbers that are both even and divisible by 3? We just add another condition to the elif statement and enclose each condition in parentheses, so that Python knows how things group together:\n\nfor i in range(1,11):\n    if i % 2 == 0:\n        print(i, \"is even\")\n    elif (i % 2 == 0) and (i % 3 == 0):\n        print(i, \"is even and divisible by 3\")\n    else:\n        print(i, \"is odd\")\n\n1 is odd\n2 is even\n3 is odd\n4 is even\n5 is odd\n6 is even\n7 is odd\n8 is even\n9 is odd\n10 is even\n\n\nWhy did this not work? The number 6 is even and divisible by 3! The reason is that the three statements (if, elif, and else) are being executed in the order that we wrote them down. That means, that Python will first check for each number whether it is even (and nothing more), and if it is, it will follow the instruction to print it and go on to the next number. So, once we arrived at 6, the programm will only check if the number is even. That is not the desired result and we have to make a little change to it. We will switch the conditions in the if and elif statements:\n\nfor i in range(1,11):\n    if (i % 2 == 0) and (i % 3 == 0):\n        print(i, \"is even and divisible by 3\")\n    elif i % 2 == 0:\n        print(i, \"is even\")\n    else:\n        print(i, \"is odd\")\n\n1 is odd\n2 is even\n3 is odd\n4 is even\n5 is odd\n6 is even and divisible by 3\n7 is odd\n8 is even\n9 is odd\n10 is even\n\n\nNow it works! Note that new never specified any conditions for the else statement. This is because whatever follows it will be executed in case none of the conditions in if or elif are met.",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Python primer</span>"
    ]
  },
  {
    "objectID": "python_primer.html#functions",
    "href": "python_primer.html#functions",
    "title": "3  A Python primer",
    "section": "3.4 Functions",
    "text": "3.4 Functions\nWith more and more experience in programming, it is likely that your code will become more and more complex. That means that it will become harder to keep track of what every piece of it is supposed to do. A good strategy to deal with this is to aim for writing code that is modular: it can be broken down into smaller units (modules) that are easier to understand. Moreover, it is sometimes necessary to reuse the same code several times. It would, however, be inefficient to write the same lines over and over again. With your code being modular you can wrap the pieces that you need in several places into a function.\nLet’s look at an example! Assume, your (fairly) complex code involves calculating the sum of the squares of two numbers. In Python, we use the + operator to calculate sums and the ** operator to raise a number to a certain power (**2 for the square of a number).\n\nx = 3\ny = 5\n\nsum_of_squares = x**2 + y**2\n\nThe variable sum_of_squares now contains the sum of squares of x=3 and y=5. We can inspect the result by calling the variable:\n\nsum_of_squares \n\n34\n\n\nNow, imagine that you would have to do the same calculation several times for different combinations of values for x and y (and always keeping in mind that this stands in for much more complex examples with several lines of code). We can code this in a function:\n\ndef func_sum_of_squares(x, y):\n    return x**2 + y**2\n\nNow, each time we want to calculate a sum of squares, we can do so by simply invoking\n\nfunc_sum_of_squares(5,4)\n\n41\n\n\nAnd, of course, we could chose a shorter name for the function as well, although I would recommend to always use function names that make clear what the function does:\n\nf = func_sum_of_squares\n\nf(5,4)\n\n41",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Python primer</span>"
    ]
  },
  {
    "objectID": "python_primer.html#libraries-youll-love",
    "href": "python_primer.html#libraries-youll-love",
    "title": "3  A Python primer",
    "section": "3.5 Libraries you’ll love",
    "text": "3.5 Libraries you’ll love\nLuckily, you don’t have to programm all functions by yourself. There is a huge community of Python programmers out there that works and collaborates on several libraries. A library is (more or less) simply a collection of certain functions (and some more, but we don’t get into this here). This means, instead of writing a function yourself, you can rely on functions that someone else has programmed.\n\n\n\n\n\n\nCaution\n\n\n\nWhether a Python library or function does actually do what it promises is another story. Popular libraries with tens of thousands of users are very trust-worthy because you can be almost sure that someone would have noticed erroneous behavior. But it is certainly possible that badly-maintained libraries contain errors. So be prudent when using the code of others.\n\n\n\n3.5.1 NumPy\nOne of the most popular Python libaries is NumPy for numerical computations. We will rely a lot on the functions in this library, especially in order to draw random samples—more on this later! To use the functions or variables from this library, they have to be imported so that you can use them. There are several ways to do this. For example, you can import the libary entirely:\n\nimport numpy\n\nNow, you can use the (approximated) value of \\(\\pi\\) stored in this library by typing\n\nnumpy.pi\n\n3.141592653589793\n\n\nA different way is to just import everything from the library by writing\n\nfrom numpy import * \n\nHere, the * stands for ‘everything’. Now, to use the value of \\(\\pi\\) we could simply type\n\npi\n\n3.141592653589793\n\n\nThis is, however discouraged for the following reason: imagine we had another library, numpy2 that also stores the value of \\(\\pi\\), but less precisely (e.g. only as 3.14). If we write\n\nfrom numpy import * \nfrom numpy2 import * \n\nWe would have imported the variables holding the value of \\(\\pi\\) from both libraries. But, because they have the same name pi. In this case, pi would equal 3.14 because we imported numpy2 last. This is confusing and shouldn’t be so! To avoid this, it is better to keep references to imported libraries explicit. In order not to have to type too much (we’re all lazy, after all), we can define an alias for the library.\n\nimport numpy as np\nnp.pi\n\n3.141592653589793\n\n\nAll functions of NumPy are now accessible with the prefix np..\n\n\n3.5.2 Pandas\n[TODO]\n\n\n3.5.3 Matplotlib\n[TODO]\n\n\n3.5.4 Summary\nYou can choose any alias when importing a library (it can even by longer than the library name) but certain conventions have emerged that you’re encouraged to follow. Importing the most commonly used Python libraries for data-science tasks (“The data science triad”), use the following:\n\nimport numpy as np # for numerical computations\nimport pandas as pd # for tabular data \nimport matplotlib.pyplot as plt # for data visualization\n\nWe will use all three of them in the following chapters and you’ll learn to love them.\n\n\n\n\n\n\nConcepts covered\n\n\n\n\nvariables\ntypes (integers, strings, lists, dictionaries)\nfunctions\nlibraries, importing and aliases",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Python primer</span>"
    ]
  },
  {
    "objectID": "randomness.html",
    "href": "randomness.html",
    "title": "4  Randomness and order",
    "section": "",
    "text": "4.1 Random draws from a bag\nLet’s try this in Python. We will use the random modul of the NumPy library:\nimport numpy as np\nrng = np.random.default_rng() # initialize a default random generator\n\nbag = range(4)\nball = rng.choice(bag)\n\nprint(ball)\n\n1\nIf this draw were really random, we would expect that each number is equally likely. We can test this by repeating this procedure again and again, tallying the result each time.\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nsamples = []\n\nfor i in range(1000):\n    ball = rng.choice(bag)\n    samples.append(ball)\n\ns = pd.Series(samples)\n\nprint(s.head(10))\nprint(s.value_counts().sort_index())\n\n0    1\n1    3\n2    2\n3    1\n4    2\n5    3\n6    2\n7    2\n8    0\n9    3\ndtype: int64\n0    234\n1    261\n2    237\n3    268\nName: count, dtype: int64\nThe number are not exactly the same but pretty close. If we would continue sampling from our bag, they would get more and more similar to one another. It is easier to understand this by visualizing it.\ns.value_counts().sort_index().plot(kind=\"bar\")\nplt.show()\nNow, what if the numbers in the bag were not just numbered, but had different colors? Let’s assume we have another bag, bag2, with 4 balls, three brown, one blue:\nbag2 = [ \"blue\", \"blue\", \"blue\", \"brown\" ]\n\nsamples2 = []\n\nfor i in range(1000):\n    ball = np.random.choice(bag2)\n    samples2.append(ball)\n\ns2 = pd.Series(samples2)\n\nprint(s2.head(10))\nprint(s2.value_counts().sort_index())\n\ns2.value_counts().sort_index().plot(kind=\"bar\")\nplt.show()\n\n0    blue\n1    blue\n2    blue\n3    blue\n4    blue\n5    blue\n6    blue\n7    blue\n8    blue\n9    blue\ndtype: object\nblue     742\nbrown    258\nName: count, dtype: int64\nThis is remarkable: by randomly (uniformly) drawing from the second bag, the frequencies of all samples approach the ratio of brown to blue balls (3:1)!",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Randomness and order</span>"
    ]
  },
  {
    "objectID": "randomness.html#composing-random-melodies",
    "href": "randomness.html#composing-random-melodies",
    "title": "4  Randomness and order",
    "section": "4.2 Composing random melodies",
    "text": "4.2 Composing random melodies\nSince this book is about music, let’s see how we can use randomness to create (a resemblance of) music. For instance, we can ‘compose’ a random melody by using only the white keys on a piano within some octave:\n\nnotes = list(\"CDEFGAB\")\nmelody = rng.choice(notes, size=10)\nprint(melody, end=\" \")\n\n['F' 'D' 'D' 'G' 'B' 'C' 'C' 'C' 'A' 'E'] \n\n\nWe composed a little melody by randomly drawing a note from the list of notes. This is also called sampling. Note that some notes repeat, showing that we sample with replacement: after each draw, the note is put back in the bag, so to speak. Of course, there are many things that we would have to generate, too, to make this a real melody. For instance, we do not know the duration of any of these notes, we don’t know the meter nor the key, we don’t know the tempo or volume, and so on. But our goal here is not to create a beautiful piece of music, but rather to show how we can use randomness to generate something.\nAs you might remember from the previous chapter, we can also write a function to do this, so that we can perform this operation (composition of a random melody) more easily, while at the same time having more control over it through its parameters. The following function does exactly this, having only one parameter that controlls the length of the melody (the number of notes to be sampled).\n\ndef melody(n):\n    notes = list(\"CDEFGAB\")\n    return rng.choice(notes, size=n)\n\nWe can now use this function to easily create random melodies of different lengths:\n\nprint(melody(7))\n\n['B' 'E' 'G' 'A' 'G' 'A' 'B']\n\n\n\nprint(melody(12))\n\n['C' 'G' 'B' 'G' 'G' 'C' 'C' 'B' 'G' 'F' 'A' 'G']",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Randomness and order</span>"
    ]
  },
  {
    "objectID": "randomness.html#synthesizing-a-corpus",
    "href": "randomness.html#synthesizing-a-corpus",
    "title": "4  Randomness and order",
    "section": "4.3 Synthesizing a corpus",
    "text": "4.3 Synthesizing a corpus\nThe functionalities introduced above allow us to synthesize an artificial corpus of melodies, here simplified as lists of pitch classes and containing varying numbers of notes.\n\nN = 4 # number of pieces in the corpus\ncorpus = [ melody(12) for _ in range(N)]\n\nThe first three melodies of our corpus are:\n\nfor mel in corpus:\n    print(mel)\n\n['F' 'B' 'E' 'F' 'C' 'G' 'G' 'C' 'C' 'F' 'D' 'D']\n['D' 'E' 'A' 'D' 'B' 'F' 'F' 'E' 'D' 'C' 'C' 'D']\n['E' 'A' 'B' 'D' 'C' 'G' 'E' 'B' 'E' 'G' 'F' 'G']\n['B' 'A' 'B' 'G' 'D' 'B' 'F' 'D' 'F' 'A' 'C' 'F']\n\n\nOf course, melodies are not always of the same length. We could vary the lenght of the melodies by creating a hand-crafted list specifying the number of notes for each melody in the corpus.\n\ncorpus = [ melody(n) for n in [10, 5, 7, 13] ]\n\nfor mel in corpus:\n    print(mel)\n\n['B' 'A' 'F' 'E' 'D' 'D' 'B' 'B' 'G' 'A']\n['D' 'A' 'D' 'D' 'G']\n['G' 'B' 'G' 'B' 'C' 'G' 'G']\n['A' 'A' 'G' 'E' 'F' 'G' 'C' 'F' 'D' 'E' 'F' 'C' 'C']\n\n\nHowever, specifying the lenghts of the melodies for a large corpus would be a very time-consuming task. In order to model the variability in length of melodies in a musical corpus, we will randomly sample them from a specified probability distribution. A good candidate for such a distribution is the Poisson distribution, that we can access from our random number generator rng.\n\nlam = 25 # average number of notes in melody\nN = 1000 # number of pieces in the corpus \n\ncorpus = [ melody(rng.poisson(lam=lam)) for _ in range(N) ]\n\nlengths = pd.Series([ len(m) for m in corpus ]).value_counts()\n\nidx = range(0, max(lengths))\nlengths = lengths.sort_index().reindex(idx).fillna(0)\nplt.bar(idx, lengths)\nplt.axvline(lam, c=\"red\")\nplt.show()\n\n\n\n\n\n\n\nFigure 4.1: Distribution of melody lengths in the corpus.”\n\n\n\n\n\nNow the variable corpus contains lists of pitch classes (aka melodies) of different lengths, most of them around the preset average value lam,1 also indicated by the vertical red line. It is moreover evident that the corpus contains rather few very short or long melodies.\nWe can, of course, not only observe the distribution of melody lengths, but also look at the overall distribution of note occurrence in the corpus:\n\ncounts = []\n\nfor m in corpus:\n    c = pd.Series(m)\n    counts.append(c)\n\npd.concat(counts).value_counts().plot(kind=\"bar\")\nplt.show()\n\n\n\n\n\n\n\nFigure 4.2: Overall note frequencies in the artificial corpus.\n\n\n\n\n\nAt this point, we should stop and celebrate. We have just written our first probabilistic model to generate melodies. Admittedly, it is not a very good model for actual melodies, for example because notes are drawn uniformly at random from the set of diatonic pitch classes using the .choice() method, which leads to the somewhat unrealistic picture in Figure 4.2. One would expect that in real melodies some notes occur more often than others and that the occurrence of a note does, for instance, also depend on the notes that come before and after it. But, in principle, these other constraints could be added to our model to make it more realistic. The point here was mainly to illustrate how artificial corpora can be generated probabilistically. This will prove useful later on because it allows us to compare real-world corpora of music against synthetic ones generated by our models.\n\n\n\n\n\n\nExercise\n\n\n\nExpand our melody model so that it also includes octave information for each pitch class in order to make it a bit more musical.",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Randomness and order</span>"
    ]
  },
  {
    "objectID": "randomness.html#pattern-search",
    "href": "randomness.html#pattern-search",
    "title": "4  Randomness and order",
    "section": "4.4 Pattern search",
    "text": "4.4 Pattern search\n\n4.4.1 Incipits\nNow that we have a corpus that we understand very well because we specified how it has been created, we can apply some simple analytical questions in order to warm up for later. For instance, we could want to have a function that allows us to search for incipits. Incipits are the beginnings of musical melodies that already to characterize themes and motives because incipits are often characteristic. For example, we would want to look for all melodies that begin with “C”, “D”, “E” and, for simplicity, we might want to pass a string like “CDE” to the function to facilitate the input.\n\nimport re \n\ndef find_incipit(incip=\"\", mel=None):\n    melody = \"\".join(mel)\n    if re.search(\"^\" + incip, melody):\n        return True\n    else:\n        return False\n\nfor m in corpus[:10]:\n    if find_incipit(incip=\"CDE\", mel=m):\n        print(\"\".join(m))\n\n\n\n4.4.2 Finals\nWe can apply a similar logic to find finals, the last notes of a melody. Instead of only allowing to search for a single note as a final, we will allow more generally to allow for a pattern that concludes a melody:\n\ndef find_finals(end=\"\", mel=None):\n    melody = \"\".join(mel)\n    if re.search(end + \"$\", melody):\n        return True \n\nfor m in corpus[:100]:\n    if find_finals(end=\"GC\", mel=m):\n        print(m)\n\n['C' 'A' 'D' 'D' 'D' 'A' 'D' 'D' 'F' 'B' 'G' 'E' 'E' 'B' 'C' 'F' 'E' 'F'\n 'C' 'B' 'B' 'B' 'E' 'F' 'G' 'D' 'A' 'D' 'A' 'D' 'G' 'C']\n['G' 'D' 'D' 'G' 'B' 'F' 'A' 'F' 'A' 'E' 'G' 'D' 'B' 'B' 'D' 'F' 'G' 'E'\n 'B' 'B' 'G' 'C' 'A' 'A' 'A' 'G' 'G' 'B' 'C' 'A' 'G' 'C']\n['A' 'D' 'A' 'E' 'F' 'F' 'B' 'E' 'B' 'F' 'G' 'E' 'C' 'A' 'G' 'A' 'G' 'D'\n 'F' 'A' 'F' 'B' 'G' 'C']\n\n\nAs you can see, all found melodies end with a falling perfect fifth form “G” to “C”.\nThe last function, find_finals(), introduced the “^” (caret) character. In the context of regular expressions, this character signifies “at the end of a string”, exactly what we needed to find finals.\n\n\n4.4.3 Patterns more generally\n\n\n\n\n\n\nTodo\n\n\n\nIntroduce regexes more flexibly and write a general pattern matcher.",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Randomness and order</span>"
    ]
  },
  {
    "objectID": "randomness.html#horrible-homophony",
    "href": "randomness.html#horrible-homophony",
    "title": "4  Randomness and order",
    "section": "4.5 Horrible Homophony",
    "text": "4.5 Horrible Homophony\nFour-part writing is a core part of Western composition history. Here, we will build a mock version of a four-part chorale by randomly generating each voice and putting them together in a table. Doing so will show you how you can create tables, which we will need later on. The most popular way to work with tables in Python is by using the pandas library. In pandas, tables are called ‘data frames’, and there is a DataFrame object to represent tables. Let’s see how we could create a four-part homophonic chorale with eight ‘chords’:\n\nimport pandas as pd\n\nn = 8\n\nchorale = pd.DataFrame({\n    \"S\" : melody(n),\n    \"A\" : melody(n),\n    \"T\" : melody(n),\n    \"B\" : melody(n)\n})\n\nThe variable chorale now stores our little composition and we can inspect it:\n\nchorale\n\n\n\n\n\n\n\n\n\nS\nA\nT\nB\n\n\n\n\n0\nE\nG\nF\nC\n\n\n1\nB\nF\nE\nG\n\n\n2\nG\nA\nG\nE\n\n\n3\nA\nD\nC\nF\n\n\n4\nE\nG\nC\nA\n\n\n5\nA\nE\nG\nF\n\n\n6\nD\nB\nE\nF\n\n\n7\nC\nB\nD\nF\n\n\n\n\n\n\n\n\nHere we have generated each voice using the melody function. We can use it to create a new function that will directly give us a new chorale of a certain length:\n\ndef chorale(n):\n    df = pd.DataFrame({\n        \"S\" : melody(n=n),\n        \"A\" : melody(n=n),\n        \"T\" : melody(n=n),\n        \"B\" : melody(n=n)\n    })\n\n    return df\n\n\nmy_chorale = chorale(n=12)\nmy_chorale\n\n\n\n\n\n\n\n\n\nS\nA\nT\nB\n\n\n\n\n0\nG\nE\nC\nC\n\n\n1\nA\nG\nE\nE\n\n\n2\nA\nD\nF\nG\n\n\n3\nG\nD\nA\nG\n\n\n4\nC\nD\nD\nD\n\n\n5\nF\nA\nA\nC\n\n\n6\nB\nC\nC\nD\n\n\n7\nE\nB\nB\nD\n\n\n8\nB\nE\nD\nF\n\n\n9\nB\nB\nG\nC\n\n\n10\nG\nB\nF\nG\n\n\n11\nG\nE\nC\nC\n\n\n\n\n\n\n\n\nIt will look a bit closer to musical notation if we transpose the data frame by using the .transpose() method:\n\nmy_chorale.transpose()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n\nS\nG\nA\nA\nG\nC\nF\nB\nE\nB\nB\nG\nG\n\n\nA\nE\nG\nD\nD\nD\nA\nC\nB\nE\nB\nB\nE\n\n\nT\nC\nE\nF\nA\nD\nA\nC\nB\nD\nG\nF\nC\n\n\nB\nC\nE\nG\nG\nD\nC\nD\nD\nF\nC\nG\nC",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Randomness and order</span>"
    ]
  },
  {
    "objectID": "randomness.html#accessing-data",
    "href": "randomness.html#accessing-data",
    "title": "4  Randomness and order",
    "section": "4.6 Accessing data",
    "text": "4.6 Accessing data\nHaving the variable my_chorale store our data frame, this is how we can access individual voices:\n\nmy_chorale[\"T\"]\n\n0     C\n1     E\n2     F\n3     A\n4     D\n5     A\n6     C\n7     B\n8     D\n9     G\n10    F\n11    C\nName: T, dtype: object\n\n\nYou can verify that it is the same ‘melody’ as above in the chorale. If we want a specific note from this voice, say the fifth one, we can access is this way:\n\nmy_chorale[\"T\"][4]\n\n'D'\n\n\nWe first select the “T” column, and then select the fifth element (remember that we start counting at 0, so we need to insert 4 to get the fifth). We can also get entire ranges of a voice:\n\nmy_chorale[\"A\"][4:8]\n\n4    D\n5    A\n6    C\n7    B\nName: A, dtype: object\n\n\nThis gives us the fifths to ninth note in the Alto voice. If we want to apply the same logic also to column ranges, we have to write it a bit differently using the .loc() method for localising data:\n\nmy_chorale.loc[1:3, \"S\":\"A\"]\n\n\n\n\n\n\n\n\n\nS\nA\n\n\n\n\n1\nA\nG\n\n\n2\nA\nD\n\n\n3\nG\nD\n\n\n\n\n\n\n\n\n.loc() takes two arguments: the rows (or row range), and the columns (or column range). We can use it to ‘slice’ our data frame in order to get specific portions of it.",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Randomness and order</span>"
    ]
  },
  {
    "objectID": "randomness.html#footnotes",
    "href": "randomness.html#footnotes",
    "title": "4  Randomness and order",
    "section": "",
    "text": "Short for the Greek letter \\(\\lambda\\) (“lambda”).↩︎",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Randomness and order</span>"
    ]
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "5  Models",
    "section": "",
    "text": "5.1 A simple example\nModels are meant to be abstract representations of (some part of) reality. They necessarily need to be simpler than reality, simple enough so that we can understand them, but close enough to reality so that they are actually useful to us.\nWhy not having more complex models that accurately represent reality? Well, first of all that is a very difficult endeavour. But, more importantly:\nSo what does that mean in practice? I will try to demonstrate this with an admittedly boring but hopefully illustrative example. Assume we want to model the entire area of this (fictional) country:\nMaybe the most simple, albeit naive approach would be to say that the total area of this country can be modeled with a square. A square is a very parsimonious model: in order to describe its area, only one parameter is needed, its side length.\nWhat’s more, we can even give a precise mathematical formula for the area of our fictional country under the square model:\n\\[M_1(a) = a^2\\]\nIn Python code, we could express this model as the following function:\ndef square_model(a):\n    return a**2\nOf course, this is not a good model of the area of the country. But one of the strengths of formal models is that they are unquivocal. There might be situations, in which the rough estimate of the square model is actually sufficient for our purpose. So why use a more complex model if the simplest one does the job?\nMost of the time, however, such a simplistic model will not suffice and we need to invest brain power to come up with a better one.1 The following model is one way to improve upon our first mode:\nThe shape of the rectangle seems to fit our country’s outline better. Hooray! As for the square model, we do have a mathematical formuly to describe our rectangle model:\n\\[M_2(a,b) = a \\cdot b\\]\nIn Python code:\ndef rectangle(a,b):\n    return a * b\nBut this improvement comes at a price: instead of having only one parameter, a, we now have two, a and b. Our model has instantly become twice as complex!\nModeling is at the core of social science and there are many good text about this topic (McElreath, 2020; Smaldino, 2017; Smaldino, 2023). In the humanities, there are fewer approaches taking modeling seriously, but they are growing in number (Finkensiep et al., forthcoming; Piotrowski, 2019).",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "models.html#a-simple-example",
    "href": "models.html#a-simple-example",
    "title": "5  Models",
    "section": "",
    "text": "No model is ever a complete recreation of reality. That would be pointless: we would have replaced a complex, incomprehensible reality with a complex, incomprehensible model. Instead, models are useful because of their simplicity. (Acerbi et al., 2022, Introduction)\n\n\n\n\n\nThe outline of a fictional country.\n\n\n\n\n\n\nA square.\n\n\n\n\n\n\n\n\n\n\n\nA rectangle.\n\n\n\n\n\n\n\n\n\n\n\n\nAcerbi, A., Mesoudi, A., & Smolla, M. (2022). Individual-based models of cultural evolution: A step-by-step guide using R. Routledge. https://acerbialberto.com/IBM-cultevo/\n\n\nBentley, R. A., Hahn, M. W., & Shennan, S. J. (2004). Random drift and culture change. Proceedings of the Royal Society of London. Series B: Biological Sciences, 271(1547), 1443–1450. https://doi.org/10.1098/rspb.2004.2746\n\n\nBishop, C. M. (2012). Model-based machine learning. Philosophical Transactions of The Royal Society A, 0222(371). https://doi.org/10.1098/rsta.2012.0222\n\n\nFarrell, S., & Lewandowsky, S. (2018). Computational Modeling of Cognition and Behavior. Cambridge University Press. https://doi.org/10.1017/CBO9781316272503\n\n\nFinkensiep, C., Neuwirth, M., & Rohrmeier, M. (forthcoming). Music Theory and Model-driven Corpus Research. In D. Shanahan, J. A. Burgoyne, & I. Quinn (Eds.), Oxford Handbook of Music and Corpus Studies. Oxford University Press.\n\n\nHoning, H. (2006). Computational Modeling of Music Cognition: A Case Study on Model Selection. Music Perception, 23(5), 365–376. https://doi.org/10.1525/mp.2006.23.5.365\n\n\nMcElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and STAN (Second). Chapman and Hall/CRC.\n\n\nPiotrowski, M. (2019). Historical Models and Serial Sources. Journal of European Periodical Studies, 4(1), 8–18. https://doi.org/10.21825/jeps.v4i1.10226\n\n\nSmaldino, P. E. (2017). Models Are Stupid, and We Need More of Them. In R. R. Vallacher, S. J. Read, & A. Nowak (Eds.), Computational Social Psychology (First, pp. 311–331). Routledge. https://doi.org/10.4324/9781315173726-14\n\n\nSmaldino, P. E. (2023). Modeling Social Behavior: Mathematical and Agent-Based Models of Social Dynamics and Cultural Evolution. Princeton University Press.",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "models.html#footnotes",
    "href": "models.html#footnotes",
    "title": "5  Models",
    "section": "",
    "text": "‘Better’ means here that it is closer to the reality we actually want to describe, while at the same time being as simple as possible. This trade-off is usually called “Occam’s razor”. Google it!↩︎",
    "crumbs": [
      "Prelude",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "chapter03.html",
    "href": "chapter03.html",
    "title": "6  Unbiased transmission",
    "section": "",
    "text": "6.1 Creating a conceptual understanding\nIt is a useful exercise to imagine first what we want to implement. This way we can check whether we have a good conceptual understanding, which will help us to write the code more clearly and make fewer mistakes. The following figure shows in the upper panel a scenario, in which there is a population of N individuals present in each generation. Those individuals are uniquely characterized by one of two traits: whether they like opera (salmon color) or not (grey). This visualization captures all assumptions we make for our first model:\nThe models throughout this book assume that invidivuals in one generation learn from individuals of (only) the previous generation by “picking” an older individual and adopting its traits according to some probabilistic rules built into the model. This means an arrow from individual n in generation t to an individual m in generation t + 1 should be interpreted as: “Individual m learns from individual m” and not the other way around (because information is flowing from generation t to t+1).\nThe lower plot shows, at each time, point the percentage of individuals having a preference for opera. A logical consequence of this process is that each transmission chain (each sequence of directly connected arrows) will pass through one and only one trait (color).\nWe can make further observations from this initial example. In the first generation, there are two individuals who do like opera and three who don’t. In subsequent generations, the proporation of these two traits changes. It is not monotonic (it goes both up and down), as the red line shows. But in the sixth generation, everything changes. Individuals in the sixth generation could inherit their trait from the one individual in generation 5 that likes opera. But because they randomly pick their ancestors, that individual is not among the ancestors. Consequently, no individual in generation 6 likes opera. This also means that, from now on, now one will ever like opera again. Opera fans have become extinct.\nWe can see that transmission of information is still going on: there are arrows between generations, so individuals still receive information from the previous generation. But nothing changes. That means that we do have transmission of information, but we would not anymore speak of it as cultural evolution, since the first fundamental criterion (see Chapter X), variation, is not fulfilled.\nNote also, that some ‘traditions’ are likewise not continued. For example, following the transmission chain of individual 0 in the first generation to individual 2 in the second generation to individual 3 in the third generation, we see that this latter individual is not picked as an ancestor by anyone from the next generation. This individual (and all of its ancestors) have no more impact on future generations. Importantly, however, we are not interested in individual fates, but rather in population-level statistics. That is why the lower plot only traces the proportion of individuals having a preference for opera.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Unbiased transmission</span>"
    ]
  },
  {
    "objectID": "chapter03.html#creating-a-conceptual-understanding",
    "href": "chapter03.html#creating-a-conceptual-understanding",
    "title": "6  Unbiased transmission",
    "section": "",
    "text": "The number of individuals per generation does not change.\nGenerations are disjunct, there is no overlap.\nIn the first generation, opera preference is randomly assigned to individuals.\nIn each subsequent generation, each individual randomly picks an ‘ancestor’ from the previous generation and blindly copies that individual’s preference for opera.\n\n\n\n\n\nA conceptual depiction of unbiased transmission.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Unbiased transmission</span>"
    ]
  },
  {
    "objectID": "chapter03.html#simulating-a-population",
    "href": "chapter03.html#simulating-a-population",
    "title": "6  Unbiased transmission",
    "section": "6.2 Simulating a population",
    "text": "6.2 Simulating a population\nWith this conceptual understanding in mind, we will now look at how we can reproduce this model of unbiased transmission. Since we are assuming that new individuals randomly pick an ancestor, we should not assume that our results will be exactly the same as above, but they should nonetheless be qualitatively similar.\n\nN = 100\nt_max = 100\n\n\n\n\n\n\n\nNote\n\n\n\nIn general, we use the variable t to designate generation counts.\n\n\nNow we create a variable population that will store the data about our simulated population. This population has either of two traits \"A\" and \"B\", with a certain probability. We store all of this in a so-called ‘data frame’, which is a somewhat fancy, Pandas-specific term for a table.\n\npopulation = pd.DataFrame(\n    {\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True)}\n)\n\nLet’s take this code apart to understand it better. From the Pandas library, which we imported as the alias pd, we create a DataFrame object. The data contained in this the data frame population is specified via a dictionary that has \"trait\" as its key and a fairly complex expression starting with the random number generator rng as its value. What this value says is, from the list [\"A\", \"B\"] choose randomly N instances with replacement (if replace were set to False, we could at most sample 2 values from the list). So, the data frame population should contain 100 randomly sampled values of A’s and B’s. Let’s confirm this by looking at the first 10 individuals of the population:\n\npopulation.head(10)\n\n\n\n\n\n\n\n\n\ntrait\n\n\n\n\n0\nA\n\n\n1\nB\n\n\n2\nB\n\n\n3\nA\n\n\n4\nA\n\n\n5\nB\n\n\n6\nA\n\n\n7\nB\n\n\n8\nA\n\n\n9\nA\n\n\n\n\n\n\n\n\nAs you can see, population stores a table (many of the 100 rows are omitted here for display reasons) and a single column called ‘trait’. The .head() method appended to the population data frame shows restricts the output to only the first 5 rows (0 through 4). Each row in the ‘trait’ column contains either the value A or B. To the left of the data frame you can see the numbers of rows explicitly spelled out. This is called the data frame’s index.\n\n\n\n\n\n\nNote\n\n\n\nA and B are just placeholder names for any of two mutually exclusive cultural traits. These could be, for example, preference for red over white whine (ignoring people who like rosé as well as people who have no preference). You see already here that this is a massive oversimplification of actual taste preferences. The point here is not to construct a plausible model but rather to gradually build up a simple one in order to understand well its inner workings.\nIt will help to pause for a moment and to think of other examples that “A” and “B” could stand for. Can you come up with a music-related one?\n\n\nFor instance, we could say that the mutually exclusive traits “A” and “B” correspond to “Individual likes opera” and “Individual doesn’t like opera”. People are often opinionated about opera, so we will stick to this example for the remainder of this part of the book. But be encouraged to try to transfer the following to different hypothetical scenarios.\n\n\n\nScene from “What’s opera, doc?” (1957).\n\n\nBack to our artificial population. We can count the number of A’s and B’s amongst the individuals as follows:\n\npopulation[\"trait\"].value_counts()\n\nB    52\nA    48\nName: trait, dtype: int64\n\n\nYou can read the above code as “From the population table, select the ‘trait’ colum and count its values.”. Since there were only two values to sample from and they were randomly (uniformly) sampled, the number of A’s and the number of B’s should be approximately equal. We can obtain their relative frequencies by adding setting the normalize keyword to True:\n\npopulation[\"trait\"].value_counts(normalize=True)\n\nB    0.52\nA    0.48\nName: trait, dtype: float64",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Unbiased transmission</span>"
    ]
  },
  {
    "objectID": "chapter03.html#tracing-cultural-change",
    "href": "chapter03.html#tracing-cultural-change",
    "title": "6  Unbiased transmission",
    "section": "6.3 Tracing cultural change",
    "text": "6.3 Tracing cultural change\nWe now create a second data frame output in which we will store the output of our model. This data frame has two columns: generation, which is the number of the simulated generation, and p which stands for “the probability of an individual of having trait A”.\n\noutput = pd.DataFrame(\n    {\n        \"generation\": np.arange(t_max, dtype=int), \n        \"p\": [np.nan] * t_max \n    }\n)\n\nThe generation column contains all numbers from 0 to t_max - 1. Because we count the numbers of generations (rather than assuming a time-continuous process), we specified that numbers in this column have to be intergers (dtype=int). The values for the p column must look cryptic. It literally says: put the np.nan value t_max times into the p colum. np.nan stands for “not a number” (from the NumPy library), since we haven’t assigned any values to this probability yet.\n\noutput.head()\n\n\n\n\n\n\n\n\n\ngeneration\np\n\n\n\n\n0\n0\nNaN\n\n\n1\n1\nNaN\n\n\n2\n2\nNaN\n\n\n3\n3\nNaN\n\n\n4\n4\nNaN\n\n\n\n\n\n\n\n\nDon’t worry that both the index and the ‘generation’ column contain all numbers from 0 to 99. We need this later when things become more involved.\nAs the saying goes, from nothing comes nothing, so we have to start somewhere, meaning that we need to assume that the initial probability of having trait A in our population is an actual number. The most sensible thing is to start with the proportions of A and B in our sampled population as a starting value.\nSo, we approximate the probability of an individual having trait A with the relative frequency of trait A in the population:\n\npopulation[\"trait\"].value_counts(normalize=True)[\"A\"]\n\n0.48\n\n\nYou already know this code from above, we just added the [\"A\"] part at the end to select only the relative frequencies of trait A. We want to set this as the value of p of the first generation. This can be achieved with the .loc (location) method:\n\noutput.loc[0, \"p\"] = population[\"trait\"].value_counts(normalize=True)[\"A\"]\n\nIn words, this reads: “Set location 0 (first row) in the p column of the output data frame to the relative frequency of the trait ‘A’ in the population.”",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Unbiased transmission</span>"
    ]
  },
  {
    "objectID": "chapter03.html#iterating-over-generations",
    "href": "chapter03.html#iterating-over-generations",
    "title": "6  Unbiased transmission",
    "section": "6.4 Iterating over generations",
    "text": "6.4 Iterating over generations\nRecall that we are trying to observe cultural change over the course of t_max = 100 generations. We thus simply repeat what we just did for the first generation: based on the relative frequencies of A’s and B’s in the previous generation, we sample the traits of 100 new individuals for the next generation.\n\nfor t in range(1, t_max):\n    # Copy the population data frame to `previous_population`\n    previous_population = population.copy()\n  \n    # Randomly copy from previous generation's individuals\n    new_population = previous_population[\"trait\"].sample(N, replace=True).to_frame()\n    \n    # Get p and put it into the output slot for this generation t\n    output.loc[t, \"p\"] = new_population[ new_population[\"trait\"] == \"A\"].shape[0] / N\n\nThis procedure assignes a probability of having trait “A” for each generation (each row of the p colum is filled now):\n\noutput.head()\n\n\n\n\n\n\n\n\n\ngeneration\np\n\n\n\n\n0\n0\n0.48\n\n\n1\n1\n0.50\n\n\n2\n2\n0.44\n\n\n3\n3\n0.49\n\n\n4\n4\n0.52\n\n\n\n\n\n\n\n\nTo make things easier, we wrap the above code in a function that we’ll call unbiased_transmission that can take different values for the population size N and number of generations t_max as parameters. The code below is exactly the same as above.\n\ndef unbiased_transmission_1(N, t_max):\n    population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True)})\n\n    output = pd.DataFrame({\"generation\": np.arange(t_max, dtype=int), \"p\": [np.nan] * t_max })\n\n    output.loc[0, \"p\"] = population[ population[\"trait\"] == \"A\"].shape[0] / N\n\n    for t in range(1, t_max):\n        # Copy the population tibble to previous_population tibble\n        previous_population = population.copy()\n    \n        # Randomly copy from previous generation's individuals\n        new_population = previous_population[\"trait\"].sample(N, replace=True).to_frame()\n        \n        # Get p and put it into the output slot for this generation t\n        output.loc[t, \"p\"] = new_population[ new_population[\"trait\"] == \"A\"].shape[0] / N\n    \n    return output\n\n\ndata_model = unbiased_transmission_1(N=100, t_max=200)\n\n\ndef plot_single_run(data_model):\n    data_model[\"p\"].plot(ylim=(0,1))\n\n\nplot_single_run(data_model)\n\n\n\n\nSingle run of the unbiased transmission model for a population of \\(N=100\\) individuals and \\(t_{max}=200\\) generations.\n\n\n\n\n\ndata_model = unbiased_transmission_1(N=10_000, t_max=200)\n\n\nplot_single_run(data_model)\n\n\n\n\nSingle run of the unbiased transmission model for a population of \\(N=10,000\\) individuals and \\(t_{max}=200\\) generations.\n\n\n\n\n\n\nNow, let’s adapt the code somewhat.\n\ndef unbiased_transmission_2(N, t_max, r_max):\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max):\n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True)})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # For each generation \n        for t in range(1,t_max):\n            # Copy individuals to previous_population DataFrame\n            previous_population = population.copy()\n\n            # Randomly compy from previous generation \n            population = population[\"trait\"].sample(N, replace=True).to_frame()\n\n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output\n\n\nunbiased_transmission_2(100, 100, 3).head()\n\n\n\n\n\n\n\n\n\ngeneration\np\nrun\n\n\n\n\n0\n0\n0.45\n0\n\n\n1\n1\n0.44\n0\n\n\n2\n2\n0.47\n0\n\n\n3\n3\n0.39\n0\n\n\n4\n4\n0.44\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhy could we append .head() to the unbiased_transmission_2 function?\n\n\n\ndata_model = unbiased_transmission_2(N=100, t_max=200, r_max=5)\n\n\ndef plot_multiple_runs(data_model):\n    groups = data_model.groupby(\"run\")\n    for _, g in groups:\n        g.index = g[\"generation\"]\n        g[\"p\"].plot(lw=.5, ylim=(0,1))\n\n    data_model.groupby(\"generation\")[\"p\"].mean().plot(c=\"k\", lw=\"1\")\n\n\nplot_multiple_runs(data_model)\n\n\n\n\nMultiple runs of the unbiased transmission model for a population of \\(N=100\\) individuals, with average (black line).\n\n\n\n\n\ndata_model = unbiased_transmission_2(N=10_000, t_max=200, r_max=5)\n\n\nplot_multiple_runs(data_model)\n\n\n\n\nMultiple runs of the unbiased transmission model for a population of \\(N=10,000\\) individuals, with average (black line).\n\n\n\n\n\ndef unbiased_transmission_3(N, p_0, t_max, r_max):\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max):\n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1 - p_0])})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # For each generation \n        for t in range(1,t_max):\n            # Copy individuals to previous_population DataFrame\n            previous_population = population\n\n            # Randomly compy from previous generation \n            population = population[\"trait\"].sample(N, replace=True).to_frame()\n\n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output\n\n\ndata_model = unbiased_transmission_3(10_000, p_0=.2, t_max=200, r_max=5)\nplot_multiple_runs(data_model)\n\n\n\n\n\n\n\n\nThe lesson here is that, if the population is large enough and individuals randomly copy from the previous generation, the average frequency of traits in the population will not significantly change.\n\n\n\n\nAcerbi, A., Mesoudi, A., & Smolla, M. (2022). Individual-based models of cultural evolution: A step-by-step guide using R. Routledge. https://acerbialberto.com/IBM-cultevo/",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Unbiased transmission</span>"
    ]
  },
  {
    "objectID": "chapter04.html",
    "href": "chapter04.html",
    "title": "7  Unbiased mutation",
    "section": "",
    "text": "Note\n\n\n\nThis chapter is based on “Chapter 2: Unbiased and biased mutation” in Acerbi et al. (2022).\n\n\n\nimport numpy as np\nrng = np.random.default_rng()\n\nimport pandas as pd\n\n\ndef unbiased_mutation(N, mu, p_0, t_max, r_max):\n    # Create an output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max):\n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1 - p_0])})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # For each generation \n        for t in range(1,t_max):\n            # Copy individuals to previous_population DataFrame\n            previous_population = population.copy()\n            \n            # Determine \"mutant\" individuals\n            mutate = rng.choice([True, False], size=N, p=[mu, 1-mu], replace=True)\n\n            # TODO: Something is off here! Changing the order of the conditions affects\n            # the result. Should be constant with random noise but converges to either A or B\n\n            # If there are \"mutants\" from A to B \n            conditionA = mutate & (previous_population[\"trait\"] == \"A\")\n            if conditionA.sum() &gt; 0:\n                population.loc[conditionA, \"trait\"] = \"B\"\n\n            # If there are \"mutants\" from B to A\n            conditionB = mutate & (previous_population[\"trait\"] == \"B\")\n            if conditionB.sum() &gt; 0:\n                population.loc[conditionB, \"trait\"] = \"A\"\n\n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output \n\n\ndef plot_multiple_runs(data_model):\n    groups = data_model.groupby(\"run\")\n    for _, g in groups:\n        g.index = g[\"generation\"]\n        g[\"p\"].plot(lw=.5, ylim=(0,1))\n\n    data_model.groupby(\"generation\")[\"p\"].mean().plot(c=\"k\", lw=\"1\")\n\n\ndata_model = unbiased_mutation(N=100, mu=.05, p_0=0.5, t_max=200, r_max=5)\nplot_multiple_runs(data_model)\n\n\n\n\n\n\n\n\n\ndata_model = unbiased_mutation(N=100, mu=.05, p_0=0.1, t_max=200, r_max=5)\nplot_multiple_runs(data_model)\n\n\n\n\n\n\n\n\n\n\n\n\nAcerbi, A., Mesoudi, A., & Smolla, M. (2022). Individual-based models of cultural evolution: A step-by-step guide using R. Routledge. https://acerbialberto.com/IBM-cultevo/",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Unbiased mutation</span>"
    ]
  },
  {
    "objectID": "biased_mutation.html",
    "href": "biased_mutation.html",
    "title": "8  Biased mutation",
    "section": "",
    "text": "import numpy as np\nrng = np.random.default_rng()\n\nimport pandas as pd\n\n\ndef plot_multiple_runs(data_model):\n    groups = data_model.groupby(\"run\")\n    for _, g in groups:\n        g.index = g[\"generation\"]\n        g[\"p\"].plot(lw=.5, ylim=(0,1))\n\n    data_model.groupby(\"generation\")[\"p\"].mean().plot(c=\"k\", lw=\"1\")\n\n\ndef biased_mutation(N, mu_b, p_0, t_max, r_max):\n    # Create the output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max):\n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1 - p_0])})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # For each generation \n        for t in range(1,t_max):\n            # Copy individuals to previous_population DataFrame\n            previous_population = population.copy()\n            \n            # Determine \"mutant\" individuals\n            mutate = rng.choice([True, False], size=N, p=[mu_b, 1-mu_b], replace=True)\n\n            # TODO: Something is off here! Changing the order of the conditions affects\n            # the result. Should be constant with random noise but converges to either A or B\n\n            # If there are \"mutants\" from B to A\n            conditionB = mutate & (previous_population[\"trait\"] == \"B\")\n            if conditionB.sum() &gt; 0:\n                population.loc[conditionB, \"trait\"] = \"A\"\n\n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output \n\n\ndata_model = biased_mutation(N = 100, mu_b = 0.05, p_0 = 0, t_max = 200, r_max = 5)\nplot_multiple_runs(data_model)\n\n\n\n\n\n\n\n\n\ndata_model = biased_mutation(N = 10000, mu_b = 0.05, p_0 = 0, t_max = 200, r_max = 5)\nplot_multiple_runs(data_model)\n\n\n\n\n\n\n\n\n\ndata_model &lt;- biased_mutation(N = 10000, mu_b = 0.1, p_0 = 0, t_max = 200, r_max = 5)\nplot_multiple_runs(data_model)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Biased mutation</span>"
    ]
  },
  {
    "objectID": "chapter05.html",
    "href": "chapter05.html",
    "title": "9  Biased transmission: direct bias",
    "section": "",
    "text": "Note\n\n\n\nThis chapter is based on “Chapter 3: Biased transmission: direct bias” in Acerbi et al. (2022).\n\n\n\nimport numpy as np\nrng = np.random.default_rng()\nimport pandas as pd\n\n\ndef plot_multiple_runs(data_model):\n    groups = data_model.groupby(\"run\")\n    for _, g in groups:\n        g.index = g[\"generation\"]\n        g[\"p\"].plot(lw=.5, ylim=(0,1))\n\n    data_model.groupby(\"generation\")[\"p\"].mean().plot(c=\"k\", lw=\"1\")\n\n\ndef biased_transmission_direct(N, s_a, s_b, p_0, t_max, r_max):\n    # Create the output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max):\n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1 - p_0])})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # For each generation \n        for t in range(1,t_max):\n            # Copy individuals to previous_population DataFrame\n            previous_population = population.copy()\n\n            # For each individual, pick a random individual from the previous generation\n            demonstrator_trait = previous_population[\"trait\"].sample(N, replace=True).reset_index()\n            \n            # Biased probabilities to copy\n            copy_a = rng.choice([True, False], size=N, replace=True, p=[s_a, 1 - s_a])\n            copy_b = rng.choice([True, False], size=N, replace=True, p=[s_b, 1 - s_b])\n\n            # If the demonstrator has trait A and the individual wants to copy A, then copy A\n            condition = copy_a & (demonstrator_trait[\"trait\"] == \"A\")\n            if condition.sum() &gt; 0:\n                population.loc[condition, \"trait\"] = \"A\"\n\n            # If the demonstrator has trait B and the individual wants to copy B, then copy B\n            condition = copy_b & (demonstrator_trait[\"trait\"] == \"B\")\n            if condition.sum() &gt; 0:\n                population.loc[condition, \"trait\"] = \"B\"\n\n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output \n\n\ndata_model = biased_transmission_direct(N=10_000, s_a=.1, s_b=0, \n                                         p_0=.01, t_max=200, r_max=5)\nplot_multiple_runs(data_model)\n\n\n\n\n\n\n\n\n\ndata_model = biased_transmission_direct(N=10_000, s_a=.6, s_b=.5, \n                                         p_0=.01, t_max=150, r_max=5)\nplot_multiple_runs(data_model)\n\n\n\n\n\n\n\n\n\ndata_model = biased_transmission_direct(N=10_000, s_a=.2, s_b=0, \n                                         p_0=.01, t_max=200, r_max=5)\nplot_multiple_runs(data_model)\n\n\n\n\n\n\n\n\n\n\n\n\nAcerbi, A., Mesoudi, A., & Smolla, M. (2022). Individual-based models of cultural evolution: A step-by-step guide using R. Routledge. https://acerbialberto.com/IBM-cultevo/",
    "crumbs": [
      "Modeling biases",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Biased transmission: direct bias</span>"
    ]
  },
  {
    "objectID": "chapter06.html",
    "href": "chapter06.html",
    "title": "10  Biased transmission: frequency-dependent indirect bias",
    "section": "",
    "text": "Note\n\n\n\nThis chapter is based on “Chapter 4: Biased transmission: frequency-dependent indirect bias” in Acerbi et al. (2022).\n\n\n\nimport numpy as np \nrng = np.random.default_rng()\n\nimport pandas as pd\n\n\ndef plot_multiple_runs(data_model):\n    groups = data_model.groupby(\"run\")\n    for _, g in groups:\n        g.index = g[\"generation\"]\n        g[\"p\"].plot(lw=.5, ylim=(0,1))\n\n    data_model.groupby(\"generation\")[\"p\"].mean().plot(c=\"k\", lw=\"1\")\n\n\nN = 100\np_0 = .5\nD = 1.\n\n\n# Create first generation\npopulation = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1-p_0])})\n\n\n# Create a DataFrame with a set of 3 randomly-picked demonstrators for each agent\n\ndemonstrators = pd.DataFrame({\n    \"dem1\" : population[\"trait\"].sample(N, replace=True).values,\n    \"dem2\" : population[\"trait\"].sample(N, replace=True).values,\n    \"dem3\" : population[\"trait\"].sample(N, replace=True).values\n})\n\n\n# Visualize the DataFrame\ndemonstrators.head()\n\n\n\n\n\n\n\n\n\ndem1\ndem2\ndem3\n\n\n\n\n0\nB\nA\nA\n\n\n1\nA\nA\nA\n\n\n2\nB\nA\nA\n\n\n3\nA\nA\nA\n\n\n4\nA\nA\nA\n\n\n\n\n\n\n\n\n\n# Get the number of A's in each 3-demonstrator combination\nnum_As = (demonstrators == \"A\").apply(sum, axis=1)\nnum_As.head()\n\n0    2\n1    3\n2    2\n3    3\n4    3\ndtype: int64\n\n\n\n# For 3-demonstrator combinations with all A's, set to A\npopulation[ num_As == 3 ] = \"A\"\n# For 3-demonstrator combinations with all B's, set to B\npopulation[ num_As == 0 ] = \"B\"\n\n\nprob_majority = rng.choice([True, False], p=[(2/3 + D/3), 1-(2/3 + D/3)], size=N, replace=True)\nprob_minority = rng.choice([True, False], p=[(1/3 + D/3), 1-(1/3 + D/3)], size=N, replace=True)\n\n\n# 3-demonstrator combinations with two As and one B\ncondition = prob_majority & (num_As == 2)\nif condition.sum() &gt; 0:\n    population.loc[condition, \"trait\"] = \"A\"\ncondition = ~prob_majority & (num_As == 2)\nif condition.sum() &gt; 0:\n    population.loc[condition, \"trait\"] = \"B\"\n\n# 3-demonstrator combinations with two B's and one A\ncondition = ~prob_minority & (num_As == 1)\nif condition.sum() &gt; 0:\n    population.loc[condition, \"trait\"] = \"A\"\ncondition = prob_minority & (num_As == 1)\nif condition.sum() &gt; 0:\n    population.loc[condition, \"trait\"] = \"B\"\n\n\ndemonstrators[\"new_trait\"] = population[\"trait\"]\ndemonstrators.head()\n\n\n\n\n\n\n\n\n\ndem1\ndem2\ndem3\nnew_trait\n\n\n\n\n0\nB\nA\nA\nA\n\n\n1\nA\nA\nA\nA\n\n\n2\nB\nA\nA\nA\n\n\n3\nA\nA\nA\nA\n\n\n4\nA\nA\nA\nA\n\n\n\n\n\n\n\n\n\ndef conformist_transmission(N, p_0, D, t_max, r_max):\n    # Create the output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max):\n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1 - p_0])})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # For each generation \n        for t in range(1,t_max):\n            demonstrators = pd.DataFrame({\n                \"dem1\" : population[\"trait\"].sample(N, replace=True).values,\n                \"dem2\" : population[\"trait\"].sample(N, replace=True).values,\n                \"dem3\" : population[\"trait\"].sample(N, replace=True).values\n            })\n\n            # Get the number of A's in each 3-demonstrator combination\n            num_As = (demonstrators == \"A\").apply(sum, axis=1)\n\n            # For 3-demonstrator combinations with all A's, set to A\n            population[ num_As == 3 ] = \"A\"\n            # For 3-demonstrator combinations with all A's, set to A\n            population[ num_As == 3 ] = \"A\"\n            # For 3-demonstrator combinations with all B's, set to B\n            population[ num_As == 0 ] = \"B\"\n\n            prob_majority = rng.choice([True, False], p=[(2/3 + D/3), 1-(2/3 + D/3)], size=N, replace=True)\n            prob_minority = rng.choice([True, False], p=[(1/3 + D/3), 1-(1/3 + D/3)], size=N, replace=True)\n\n            # 3-demonstrator combinations with two As and one B\n            condition = prob_majority & (num_As == 2)\n            if condition.sum() &gt; 0:\n                population.loc[condition, \"trait\"] = \"A\"\n            condition = ~prob_majority & (num_As == 2)\n            if condition.sum() &gt; 0:\n                population.loc[condition, \"trait\"] = \"B\"\n\n            # 3-demonstrator combinations with two B's and one A\n            condition = prob_minority & (num_As == 1)\n            if condition.sum() &gt; 0:\n                population.loc[condition, \"trait\"] = \"A\"\n            condition = ~prob_minority & (num_As == 1)\n            if condition.sum() &gt; 0:\n                population.loc[condition, \"trait\"] = \"B\"\n            \n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output\n\n\ndata_model = conformist_transmission(N=1_000, p_0 = 0.5, D = 1, t_max = 50, r_max = 10)\nplot_multiple_runs(data_model)\n\n\n\n\n\n\n\n\n\n\n\n\nAcerbi, A., Mesoudi, A., & Smolla, M. (2022). Individual-based models of cultural evolution: A step-by-step guide using R. Routledge. https://acerbialberto.com/IBM-cultevo/",
    "crumbs": [
      "Modeling biases",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Biased transmission: frequency-dependent indirect bias</span>"
    ]
  },
  {
    "objectID": "chapter07.html",
    "href": "chapter07.html",
    "title": "11  Biased transmission: influencer-based indirect bias",
    "section": "",
    "text": "Note\n\n\n\nThis chapter is based on “Chapter 5: Biased transmission: demonstrator-based indirect bias” in Acerbi et al. (2022).\n\n\nInstead of calling them demonstrators, we will call them influencers.\n\n\n\nAn influencer on a popular video platform.\n\n\n\nimport numpy as np \nrng = np.random.default_rng()\n\nimport pandas as pd\n\n\ndef plot_multiple_runs(data_model):\n    groups = data_model.groupby(\"run\")\n    for _, g in groups:\n        g.index = g[\"generation\"]\n        g[\"p\"].plot(lw=.5, ylim=(0,1))\n\n    data_model.groupby(\"generation\")[\"p\"].mean().plot(c=\"k\", lw=\"1\")\n\n\nN = 100\np_0 = 0.5\np_s = 0.05\n\n\npopulation = pd.DataFrame({\n    \"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1-p_0]),\n    \"status\": rng.choice([\"high\", \"low\"], size=N, replace=True, p=[p_s, 1-p_s])\n})\n\n\npopulation.head()\n\n\n\n\n\n\n\n\n\ntrait\nstatus\n\n\n\n\n0\nB\nlow\n\n\n1\nA\nlow\n\n\n2\nB\nlow\n\n\n3\nA\nlow\n\n\n4\nA\nlow\n\n\n\n\n\n\n\n\n\np_low = 0.01\np_influencer = np.ones(N)\np_influencer[ population[\"status\"] == \"low\" ] = p_low\n\n\nif sum(p_influencer) &gt; 0:\n    ps = p_influencer / p_influencer.sum()\n    influencer_index = rng.choice(np.arange(N), size=N, p=ps, replace=True)\n    population[\"trait\"] = population.loc[influencer_index, \"trait\"].values\n\n\ndef biased_transmission_influencer(N, p_0, p_s, p_low, t_max, r_max):\n    # Create the output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n    \n    for r in range(r_max):\n            # Create first generation\n            population = pd.DataFrame({\n                \"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1-p_0]),\n                \"status\": rng.choice([\"high\", \"low\"], size=N, replace=True, p=[p_s, 1-p_s])\n            })\n            \n            # Assign copying probabilities based on individuals' status\n            p_influencer = np.ones(N)\n            p_influencer[population[\"status\"] == \"low\"] = p_low\n            \n            # Add first generation's p for run r\n            output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n            \n            for t in range(1, t_max):\n                # Copy individuals to previous_population DataFrame\n                previous_population = population.copy()\n                \n                # Copy traits based on status\n                if sum(p_influencer) &gt; 0:\n                    ps = p_influencer / p_influencer.sum()\n                    influencer_index = rng.choice(np.arange(N), size=N, p=ps, replace=True)\n                    population[\"trait\"] = population.loc[influencer_index, \"trait\"].values\n                \n                # Get p and put it into output slot for this generation t and run r\n                output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n                \n    return output\n\n\ndata_model = biased_transmission_influencer(N=100, p_s=0.05, p_low=0.0001, p_0=0.5, t_max=50, r_max=10)\n\n\nplot_multiple_runs(data_model)\n\n\n\n\n\n\n\n\n\ndata_model = biased_transmission_influencer(N=10_000, p_s=0.005, p_low=0.0001, p_0=0.5, t_max=200, r_max=10)\nplot_multiple_runs(data_model)\n\n\n\n\n\n\n\n\n\ndef biased_transmission_influencer_2(N, p_0, p_s, p_low, t_max, r_max):\n    # Create the output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n    \n    ... # TODO\n    \n    return output\n\n\ndata_model = biased_transmission_influencer_2(N=100, p_s=0.1, p_low=0.0001, p_0=0.5, t_max=50, r_max=50)\n\n\n\n\n\nAcerbi, A., Mesoudi, A., & Smolla, M. (2022). Individual-based models of cultural evolution: A step-by-step guide using R. Routledge. https://acerbialberto.com/IBM-cultevo/",
    "crumbs": [
      "Modeling biases",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Biased transmission: influencer-based indirect bias</span>"
    ]
  },
  {
    "objectID": "chapter08.html",
    "href": "chapter08.html",
    "title": "12  Vertical and horizontal cultural transmission",
    "section": "",
    "text": "import numpy as np \nrng = np.random.default_rng()\n\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef plot_multiple_runs(data_model):\n    groups = data_model.groupby(\"run\")\n    for _, g in groups:\n        g.index = g[\"generation\"]\n        g[\"p\"].plot(lw=.5, ylim=(0,1))\n\n    data_model.groupby(\"generation\")[\"p\"].mean().plot(c=\"k\", lw=\"1\")\n\n\ndef vertical_transmission(N, p_0, b, t_max, r_max):\n    # Create the output DataFrame\n    output = pd.DataFrame({\n        \"generation\" : np.tile(np.arange(t_max), r_max),\n        \"p\" : [ np.nan ] * t_max * r_max,\n        \"run\" : np.repeat(np.arange(r_max), t_max)\n    })\n\n    for r in range(r_max): \n        # Create first generation\n        population = pd.DataFrame({\"trait\": rng.choice([\"A\", \"B\"], size=N, replace=True, p=[p_0, 1 - p_0])})\n\n        # Add first generation's p for run r\n        output.loc[ r * t_max, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n        # # For each generation \n        for t in range(1, t_max): \n            # Copy individuals to previous_population DataFrame\n            previous_population = population.copy()\n\n            # randomly pick mothers and fathers\n            mother = previous_population[\"trait\"].sample(N, replace=True).reset_index(drop=True)\n            father = previous_population[\"trait\"].sample(N, replace=True).reset_index(drop=True)\n\n            # prepare next generation\n            population = pd.DataFrame({\"trait\": [np.nan] * N })\n\n            # Both parents are A, thus child adopts A\n            both_A = (mother == \"A\") & (father == \"A\")\n            # if sum(both_A) &gt; 0:\n            population.loc[both_A,\"trait\"] = \"A\"\n\n            # Both parents are A, thus child adopts A\n            both_B = (mother == \"B\") & (father == \"B\")\n            # if sum(both_B) &gt; 0:\n            population.loc[both_B,\"trait\"] = \"B\"\n\n            # If any empty NA slots are present (i.e. one A and one B parent) they adopt A with probability b\n            remaining = rng.choice([\"A\", \"B\"], size=population[\"trait\"].isna().sum(), replace=True, p=[b, 1 - b])\n            population.loc[population[\"trait\"].isna(),\"trait\"] = remaining\n            \n            # Get p and put it into output slot for this generation t and run r\n            output.loc[r * t_max + t, \"p\"] = population[ population[\"trait\"] == \"A\" ].shape[0] / N\n\n    return output \n\n\ndata_model = vertical_transmission(N=10_000, p_0=0.01, b=0.6,t_max=50, r_max=5)\n\nplot_multiple_runs(data_model)\n\n\n\n\n\n\n\n\n\ndata_model = vertical_transmission(N=10_000, p_0=0.1, b=0.5,t_max=50, r_max=5)\nplot_multiple_runs(data_model)\n\n\n\n\n\n\n\n\n\ndef vertical_horizontal_transmission(N, p_0, b, n, g, t_max, r_max):\n    # Create an empty dataframe for the output\n    output = pd.DataFrame({\n        'generation': np.tile(np.arange(1, t_max + 1), r_max),\n        'p': np.full(t_max * r_max, np.nan),\n        'run': np.repeat(np.arange(1, r_max + 1), t_max)\n    })\n    \n    for r in range(r_max):\n        # Create first generation\n        population = pd.DataFrame({\n            'trait': np.random.choice(['A', 'B'], size=N, p=[p_0, 1 - p_0])\n        })\n        \n        # Add first generation's p for run r\n        output.loc[(output['generation'] == 1) & (output['run'] == r + 1), 'p'] = (population['trait'] == 'A').sum() / N\n        \n        for t in range(1, t_max):\n            # Vertical transmission --------------------------------------------------\n            \n            # Copy individuals to previous_population dataframe\n            previous_population = population.copy()\n            \n            # Randomly pick mothers and fathers\n            mother = np.random.choice(previous_population['trait'], N, replace=True)\n            father = np.random.choice(previous_population['trait'], N, replace=True)\n            \n            # Prepare next generation\n            population = pd.DataFrame({'trait': [np.nan] * N})\n            \n            # Both parents are A, thus child adopts A\n            both_A = (mother == 'A') & (father == 'A')\n            population.loc[both_A, 'trait'] = 'A'\n            \n            # Both parents are B, thus child adopts B\n            both_B = (mother == 'B') & (father == 'B')\n            population.loc[both_B, 'trait'] = 'B'\n            \n            # If any empty NA slots (i.e. one A and one B parent) are present\n            if population['trait'].isna().any():\n                # They adopt A with probability b\n                population.loc[population['trait'].isna(), 'trait'] = np.random.choice(\n                    ['A', 'B'],\n                    size=population['trait'].isna().sum(),\n                    p=[b, 1 - b]\n                )\n            \n            # Horizontal transmission ------------------------------------------------\n            \n            # Previous_population are children before horizontal transmission\n            previous_population = population.copy()\n            \n            # N_B = number of Bs\n            N_B = (previous_population['trait'] == 'B').sum()\n            \n            # If there are B individuals to switch, and n is not zero\n            if N_B &gt; 0 and n &gt; 0:\n                # For each B individual...\n                for i in range(N_B):\n                    # Pick n demonstrators\n                    demonstrator = np.random.choice(previous_population['trait'], n, replace=True)\n                    # Get probability g\n                    copy = np.random.choice([True, False], size=n, p=[g, 1 - g])\n                    \n                    # If any demonstrators with A are to be copied\n                    if (demonstrator == 'A').any() & copy.any():\n                        # The B individual switches to A\n                        b_indices = previous_population[previous_population['trait'] == 'B'].index\n                        population.loc[b_indices[i], 'trait'] = 'A'\n            \n            # Get p and put it into output slot for this generation t and run r\n            output.loc[(output['generation'] == t + 1) & (output['run'] == r + 1), 'p'] = (population['trait'] == 'A').sum() / N\n    \n    return output\n\n\nvertical_horizontal_transmission(N=1000, p_0=0.01, b=0.5, n=5, g=0.1, t_max=10, r_max=1)\n\n\n\n\n\n\n\n\n\ngeneration\np\nrun\n\n\n\n\n0\n1\n0.011\n1\n\n\n1\n2\n0.025\n1\n\n\n2\n3\n0.076\n1\n\n\n3\n4\n0.176\n1\n\n\n4\n5\n0.394\n1\n\n\n5\n6\n0.645\n1\n\n\n6\n7\n0.797\n1\n\n\n7\n8\n0.872\n1\n\n\n8\n9\n0.934\n1\n\n\n9\n10\n0.957\n1\n\n\n\n\n\n\n\n\n\ndata_model = vertical_horizontal_transmission(N=5_000, p_0=0.01, b=0.5, n=5, g=0.1, t_max=50, r_max=2)\nplot_multiple_runs(data_model)",
    "crumbs": [
      "Modeling biases",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Vertical and horizontal cultural transmission</span>"
    ]
  },
  {
    "objectID": "chapter09.html",
    "href": "chapter09.html",
    "title": "13  The multiple traits model",
    "section": "",
    "text": "13.1 Introducing innovation",
    "crumbs": [
      "Modeling biases",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The multiple traits model</span>"
    ]
  },
  {
    "objectID": "pitches.html",
    "href": "pitches.html",
    "title": "14  The usage of pitches and intervals",
    "section": "",
    "text": "“Cross-cultural data shows musical scales evolved to maximise imperfect fifths” McBride & Tlusty (2020)\n“The line of fifths and the co-evolution of tonal pitch-classes” (Moss et al., 2022)\n\n\n\n\n\nMcBride, J. M., & Tlusty, T. (2020). Cross-cultural data shows musical scales evolved to maximise imperfect fifths. http://arxiv.org/abs/1906.06171\n\n\nMoss, F. C., Neuwirth, M., & Rohrmeier, M. (2022). The line of fifths and the co-evolution of tonal pitch-classes. Journal of Mathematics and Music, 1–25. https://doi.org/10.1080/17459737.2022.2044927",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The usage of pitches and intervals</span>"
    ]
  },
  {
    "objectID": "folk_tunes.html",
    "href": "folk_tunes.html",
    "title": "15  Folk tune complexity",
    "section": "",
    "text": "“The role of population size in folk tune complexity” (Street et al., 2022)\n\n\n\n\nStreet, S., Eerola, T., & Kendal, J. R. (2022). The role of population size in folk tune complexity. Humanities and Social Sciences Communications, 9(1), 1–12. https://doi.org/10.1057/s41599-022-01139-y",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Folk tune complexity</span>"
    ]
  },
  {
    "objectID": "communities.html",
    "href": "communities.html",
    "title": "16  Music communities",
    "section": "",
    "text": "“Phylogenetic reconstruction of the cultural evolution of electronic music via dynamic community detection (1975–1999)” (Youngblood et al., 2021)\n\n\n\n\nYoungblood, M., Baraghith, K., & Savage, P. E. (2021). Phylogenetic reconstruction of the cultural evolution of electronic music via dynamic community detection (1975–1999). Evolution and Human Behavior. https://doi.org/10.1016/j.evolhumbehav.2021.06.002",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Music communities</span>"
    ]
  },
  {
    "objectID": "style_evolution.html",
    "href": "style_evolution.html",
    "title": "17  Style evolution",
    "section": "",
    "text": "“Statistical Evolutionary Laws in Music Styles” (Nakamura & Kaneko, 2019)\n“Investigating style evolution of Western classical music: A computational approach” (Weiß et al., 2019)\n\n\n\n\n\nNakamura, E., & Kaneko, K. (2019). Statistical evolutionary laws in music styles. Nature Scientific Reports, 9(1), 15993. https://doi.org/10.1038/s41598-019-52380-6\n\n\nWeiß, C., Mauch, M., Dixon, S., & Müller, M. (2019). Investigating style evolution of Western classical music: A computational approach. Musicae Scientiae, 23(4), 486–507. https://doi.org/10.1177/1029864918757595",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Style evolution</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "18  Conclusion",
    "section": "",
    "text": "18.1 What can cultural evolution tell us about music",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#what-is-the-role-of-models-for-musicology",
    "href": "conclusion.html#what-is-the-role-of-models-for-musicology",
    "title": "18  Conclusion",
    "section": "18.2 What is the role of models for musicology",
    "text": "18.2 What is the role of models for musicology",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#avenues-for-future-research",
    "href": "conclusion.html#avenues-for-future-research",
    "title": "18  Conclusion",
    "section": "18.3 Avenues for future research",
    "text": "18.3 Avenues for future research",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Acerbi, A., Mesoudi, A., & Smolla, M. (2022). Individual-based models of cultural evolution: A\nstep-by-step guide using R. Routledge. https://acerbialberto.com/IBM-cultevo/\n\n\nAppiah, K. A. (2018). The Lies that Bind:\nRethinking Identity. W. W. Norton &\nCompany.\n\n\nAunger, R. (2001). Darwinizing Culture: The\nStatus of Memetics as a Science.\nOxford University Press, USA. http://gen.lib.rus.ec/book/index.php?md5=7329e2aa9adcddfed967088219426193\n\n\nBentley, R. A., Hahn, M. W., & Shennan, S. J. (2004). Random drift\nand culture change. Proceedings of the Royal Society of London.\nSeries B: Biological Sciences, 271(1547), 1443–1450. https://doi.org/10.1098/rspb.2004.2746\n\n\nBishop, C. M. (2012). Model-based machine learning. Philosophical\nTransactions of The Royal Society A, 0222(371). https://doi.org/10.1098/rsta.2012.0222\n\n\nBlackmore, S. (2000). The Meme Machine.\nOxford University Press.\n\n\nBoyd, R., & Richerson, P. J. (1985). Culture and the Evolutionary Process. The\nUniversity of Chicago Press.\n\n\nCavalli-Sforza, L. L., & Feldman, M. W. (1981). Cultural\nTransmission and Evolution.\nPrinceton University Press.\n\n\nCross, I. (2016). The nature of music and its evolution. In S. Hallam,\nI. Cross, & M. Thaut (Eds.), The Oxford Handbook of\nMusic Psychology (2nd ed., pp. 1–20). Oxford\nUniversity Press. https://doi.org/10.1093/oxfordhb/9780199298457.013.0001\n\n\nDawkins, R. (1976). The Selfish Gene. Oxford\nUniversity Press.\n\n\nFarrell, S., & Lewandowsky, S. (2018). Computational\nModeling of Cognition and\nBehavior. Cambridge University Press. https://doi.org/10.1017/CBO9781316272503\n\n\nFinkensiep, C., Neuwirth, M., & Rohrmeier, M. (forthcoming). Music\nTheory and Model-driven Corpus\nResearch. In D. Shanahan, J. A. Burgoyne, & I. Quinn (Eds.),\nOxford Handbook of Music and Corpus\nStudies. Oxford University Press.\n\n\nGjerdingen, R. O. (2007). Music in the Galant\nStyle. Oxford University Press.\n\n\nHoning, H. (2006). Computational Modeling of Music\nCognition: A Case Study on Model\nSelection. Music Perception, 23(5), 365–376. https://doi.org/10.1525/mp.2006.23.5.365\n\n\nHoning, H. (2018). On the biological basis of musicality. Annals of\nthe New York Academy of Sciences. https://doi.org/10.1111/nyas.13638\n\n\nHowe, C. J., & Windram, H. F. (2011). Phylomemetics beyond the\nGene. PLOS Biology, 9(5), e1001069. https://doi.org/10.1371/journal.pbio.1001069\n\n\nInternational Folk Music Council. (1955). Resolutions. Journal of\nthe International Folk Music Council, 7, 23–23. https://www.jstor.org/stable/834530\n\n\nJan, S. (2016). The Memetics of Music: A\nNeo-Darwinian View of Musical Structure and Culture.\nRoutledge.\n\n\nKarpeles, M. (1955). Definition of Folk Music. Journal\nof the International Folk Music Council, 7, 6–7. https://doi.org/10.2307/834518\n\n\nMcBride, J. M., & Tlusty, T. (2020). Cross-cultural data shows\nmusical scales evolved to maximise imperfect fifths. http://arxiv.org/abs/1906.06171\n\n\nMcElreath, R. (2020). Statistical Rethinking: A\nBayesian Course with Examples in R and\nSTAN (Second). Chapman and Hall/CRC.\n\n\nMeyer, L. B. (1989). Style and Music.\nTheory, History, and\nIdeology. University of Chicago Press.\n\n\nMorley, I. (2013). The Prehistory of\nMusic. Human Evolution,\nArchaeology, and the Origins of\nMusicality. Oxford University Press.\n\n\nMoss, F. C., Neuwirth, M., & Rohrmeier, M. (2022). The line of\nfifths and the co-evolution of tonal pitch-classes. Journal of\nMathematics and Music, 1–25. https://doi.org/10.1080/17459737.2022.2044927\n\n\nNakamura, E., & Kaneko, K. (2019). Statistical evolutionary laws in\nmusic styles. Nature Scientific Reports, 9(1), 15993.\nhttps://doi.org/10.1038/s41598-019-52380-6\n\n\nPinker, S. (1997). How the mind works. Norton.\n\n\nPiotrowski, M. (2019). Historical Models and Serial\nSources. Journal of European Periodical Studies,\n4(1), 8–18. https://doi.org/10.21825/jeps.v4i1.10226\n\n\nSavage, P. E. (2019). Cultural evolution of music. Palgrave\nCommunications, 5(1), 1–16. https://doi.org/10.1057/s41599-019-0221-1\n\n\nSmaldino, P. E. (2017). Models Are Stupid, and We\nNeed More of Them. In R. R. Vallacher, S. J. Read,\n& A. Nowak (Eds.), Computational Social\nPsychology (First, pp. 311–331). Routledge. https://doi.org/10.4324/9781315173726-14\n\n\nSmaldino, P. E. (2023). Modeling Social Behavior:\nMathematical and Agent-Based Models of\nSocial Dynamics and Cultural Evolution.\nPrinceton University Press.\n\n\nStreet, S., Eerola, T., & Kendal, J. R. (2022). The role of\npopulation size in folk tune complexity. Humanities and Social\nSciences Communications, 9(1), 1–12. https://doi.org/10.1057/s41599-022-01139-y\n\n\nTomlinson, G. (2018). A Million Years of\nMusic. Princeton University Press. https://press.princeton.edu/books/paperback/9781890951528/a-million-years-of-music\n\n\nWallin, N. L., Merker, B., & Brown, S. (Eds.). (2001). The\nOrigins of Music. MIT Press.\n\n\nWeiß, C., Mauch, M., Dixon, S., & Müller, M. (2019). Investigating\nstyle evolution of Western classical music: A\ncomputational approach. Musicae Scientiae, 23(4),\n486–507. https://doi.org/10.1177/1029864918757595\n\n\nYoungblood, M., Baraghith, K., & Savage, P. E. (2021). Phylogenetic\nreconstruction of the cultural evolution of electronic music via dynamic\ncommunity detection (1975–1999). Evolution and Human Behavior.\nhttps://doi.org/10.1016/j.evolhumbehav.2021.06.002\n\n\nYoungblood, M., Ozaki, Y., & Savage, P. E. (forthcoming). Cultural\nevolution and music. In J. Tehrani, J. R. Kendal, & R. L. Kendal\n(Eds.), Oxford Handbook of Cultural\nEvolution. Oxford University Press. https://psyarxiv.com/xsb7v",
    "crumbs": [
      "References"
    ]
  }
]